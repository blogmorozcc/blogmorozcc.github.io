[{"content":"What is Docker and Docker Compose Docker and Docker Compose are tools designed to simplify the process of managing and deploying applications in containers. Hereâ€™s an overview of each, along with their differences and key benefits.\nDocker Definition:\nDocker is a platform that uses containerization to deploy, manage, and run applications. Containers are lightweight, portable, and consistent environments that include everything needed to run a piece of software, such as the code, runtime, system tools, libraries, and settings.\nKey Benefits:\nConsistency: Docker ensures that software will run the same regardless of where it\u0026rsquo;s deployed because containers encapsulate all dependencies. Isolation: Containers run in isolated environments, which makes it easier to manage dependencies and avoid conflicts. Portability: Containers can run on any system that supports Docker, including on-premises servers, public clouds, and personal machines. Efficiency: Docker containers are lightweight and have lower overhead compared to traditional virtual machines. Scalability: Docker makes it easy to scale applications horizontally by running multiple containers across different hosts. Docker Compose Definition:\nDocker Compose is a tool for defining and running multi-container Docker applications. It uses a YAML file to configure the application\u0026rsquo;s services, networks, and volumes, allowing you to manage multiple containers as a single application.\nKey Benefits:\nSimplified Configuration: Docker Compose uses a single YAML file (docker-compose.yml) to configure all your application\u0026rsquo;s services, making it easier to manage and understand. Multi-Container Management: It allows you to define, run, and manage multiple interconnected containers with a single command (docker-compose up). Environment Consistency: Ensures consistent environments across different stages of development, testing, and production by using the same configuration file. Networking: Automatically sets up a network so that the containers can communicate with each other without additional configuration. Volume Management: Simplifies the setup and management of data volumes that can be shared between containers. Differences Scope: Docker is used for single-container applications, whereas Docker Compose is designed for applications consisting of multiple containers. Usage: Docker commands (docker run, docker build, etc.) are used to manage individual containers, while Docker Compose commands (docker-compose up, docker-compose down, etc.) manage entire multi-container applications defined in a YAML file. Configuration: Docker uses Dockerfiles to define container images, while Docker Compose uses a docker-compose.yml file to define multi-container applications, including their networks and volumes. Combined Key Benefits\nStreamlined Development: Together, Docker and Docker Compose allow for rapid and consistent development, testing, and deployment of applications. Reproducibility: Both tools ensure that the environment in which the application runs is consistent across different stages, reducing the \u0026ldquo;it works on my machine\u0026rdquo; problem. Simplified CI/CD: Integration with CI/CD pipelines becomes easier, as containers can be used to run tests and deploy applications in a consistent environment. Resource Efficiency: Containers share the same OS kernel and can be more resource-efficient than virtual machines. Practical example Suppose you have a web application with the following components:\nA web server (e.g., Nginx) An application server (e.g., Node.js) A database (e.g., PostgreSQL) With Docker:\nYou create individual Dockerfiles for each component to containerize them. With Docker Compose:\nYou create a docker-compose.yml file to define all three services and their interactions. You can bring up the entire stack with a single command (docker-compose up). Docker installation First of all update system packages to the latest vesions:\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade Then install necessary packages:\nsudo apt install apt-transport-https ca-certificates curl software-properties-common gnupg2 Add GPG public key for official Docker repository:\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add - Add the Docker debian repository to apt package manager sources list:\necho \u0026#34;deb [arch=amd64] https://download.docker.com/linux/debian $(lsb_release -cs) stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list Update package indexes and install Docker Community Edition:\nsudo apt update \u0026amp;\u0026amp; sudo apt install docker-ce Enable and start the docker systemd daemon:\nsudo systemctl enable --now docker Finally, verify the installation by running demo hello-world container:\nsudo docker run hello-world Docker Compose installation Download the latest release from official repository:\nsudo curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose Add execution permissions to the downloaded binary:\nsudo chmod +x /usr/local/bin/docker-compose Verify the installition by printing out Docker Compose version:\ndocker-compose --version In addition, you can create a test docker-compose.yml file with a couple of demo services (for example web server and database):\n1 2 3 4 5 6 7 8 9 10 version: \u0026#39;3\u0026#39; services: web: image: nginx ports: - \u0026#34;80:80\u0026#34; database: image: postgres environment: POSTGRES_PASSWORD: example And then to start it use the command:\ndocker compose up Conclusion This example illustrates how to install Docker and Docker Compose on Debian system to complement each other in developing, deploying, and managing applications efficiently.\n","date":"2024-06-03T00:00:00Z","image":"http://localhost:1313/post/what-is-docker-and-docker-compose-and-how-to-install-and-use-it-on-debian/header_hub514da7001493284666285be454a18d2_432280_120x120_fill_box_smart1_3.png","permalink":"http://localhost:1313/post/what-is-docker-and-docker-compose-and-how-to-install-and-use-it-on-debian/","title":"What is Docker and Docker Compose and how to install and use it on Debian"},{"content":"Introduction Stable Diffusion is a deep learning, text-to-image model developed by Stability AI. It is primarily used to generate detailed images based on text prompts. The model belongs to the class of generative models called diffusion models, which iteratively denoise a random signal to produce an image. AUTOMATIC1111 refers to a popular web-based user interface (UI) implementation for interacting with Stable Diffusion. Developed by an individual or group under the pseudonym \u0026ldquo;AUTOMATIC1111\u0026rdquo;, it provides a robust and user-friendly way to utilize Stable Diffusion\u0026rsquo;s capabilities.\nWhy I run AUTOMATIC1111 inside VM In one of my previous articles I mentioned that I use dual AMD-GPU custom desktop system with Arch Linux as main host OS. Personally for me, running AUTOMATIC1111 inside a KVM-based virtual machine with AMD GPU passthrough offers several key benefits:\nPortability. As I use Arch Linux as my main host OS on different computers it is sometimes can be tricky to manage dependencies that are required to run AUTOMATIC1111. For example, at the time of writing this article (02.06.2024), AUTOMATIC1111 requires Python 3.10 and the bleeding edge Python version in the official arch repositories is 3.12. In case of running AUTOMATIC1111 inside a VM I can setup the dependencies once inside that VM, and I don\u0026rsquo;t have to worry about messing the dependencies on each Arch update.\nSnapshot and Restore. As I have a VM storage as single *.qcow2 file, I can backup it, transfer to my another machine or server in my homelab. It\u0026rsquo;s also easy to keep my checkpoints, loras, etc inside a single VM, and in case I need to transfer AUTOMATIC1111 installation to another host machine, I need only to copy VM backup. There is no need to do the dependencies installation, models setup every time.\nWhy I need AUTOMATIC1111 AUTOMATIC1111 software is crucial for me as I develop free and open source SDAI - Stable Diffusion Android client application that can connect to any AUTOMATIC1111 server or other supported AI Image generation provider. I need many different isolated AUTOMATIC1111 instances to perform testing of my android application.\nInstallation Create a new Linux VM First we need to create a new Linux VM with GPU Passthrough of PCI devices to that VM. I already covered creation of VM in the article \u0026ldquo;GPU PCI passthrough to Windows KVM on Arch Linux\u0026rdquo;, but this time I will use Ubuntu Sever 22.04 LTS as a OS for Guest VM. I have chosen Ubuntu Server 22.04 for the guest OS because for the moment of writing this article (02.06.2024) this is the latest release that is supported by proprietary AMD ROCM driver that is needed to run AI on the power of GPU.\nUpdate the OS packages After installing the OS first thing you need to do is to update the system packages to the latest available versions.\n1 2 sudo apt update sudo apt upgrade Install needed dependencies sudo apt install -y git python3-pip python3-venv python3-dev libstdc++-12-dev libgl1-mesa-glx Install AMD ROCM driver I used official instructions from the AMD documentation to install the ROCM driver.\nFirst, install headers and extras for the current kernel:\nsudo apt install -y \u0026#34;linux-headers-$(uname -r)\u0026#34; \u0026#34;linux-modules-extra-$(uname -r)\u0026#34; Then, ensure your current user is a part of video and render groups. To add the current user to the groups use command:\nsudo usermod -a -G render,video $LOGNAME Download installer deb package and install it:\nwget https://repo.radeon.com/amdgpu-install/6.1.1/ubuntu/jammy/amdgpu-install_6.1.60101-1_all.deb sudo dpkg -i amdgpu-install_6.1.60101-1_all.deb Install the DKMS module and rocm packages:\nsudo apt update sudo apt install amdgpu-dkms rocm Finally, reboot the VM:\nsudo reboot Install AUTOMATIC1111 software The most easy and convinient way is just to clone the official git repository. After cloning, navigate to the cloned directory.\ngit clone https://github.com/AUTOMATIC1111/stable-diffusion-webui cd stable-diffusion-webui Setup Python virtual environment:\npython3 -m venv venv source venv/bin/activate Install python dependencies needed by AUTOMATIC1111:\npip3 install -r requirements.txt Uninstall generic torch dependencies and replace them with ROCM:\npip3 uninstall torch torchvision pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0 Create custom AUTOMATIC1111 launch script I will use nano to create a new file nano launch.sh.\n1 2 3 4 5 6 7 8 9 #!/bin/sh source venv/bin/activate export HSA_OVERRIDE_GFX_VERSION=10.3.0 export HIP_VISIBLE_DEVICES=0 export PYTORCH_HIP_ALLOC_CONF=garbage_collection_threshold:0.8,max_split_size_mb:512 python3 launch.py --api --listen --enable-insecure-extension-access --opt-sdp-attention Then save the file Ctrl + O and exit nano Ctrl + X.\nLaunch AUTOMATIC1111 Every time you need to launch AUTOMATIC1111 navigate to the cloned stable-diffusion-webui directory and launch created launch.sh script like in the following example:\ncd stable-diffusion-webui bash launch.sh As the result, you will see that AUTOMATIC1111 running successfully.\nConclusion Stable Diffusion is a powerful model for generating images from text descriptions, and AUTOMATIC1111 is a user-friendly interface that makes it easier to use Stable Diffusion\u0026rsquo;s capabilities effectively. Together, they enable a wide range of creative and practical applications in the realm of generative art and image synthesis. By leveraging a KVM-based virtual machine with AMD GPU passthrough, you can achieve a powerful, secure, and flexible environment for running AUTOMATIC1111 and harnessing the capabilities of Stable Diffusion efficiently.\nReference Ubuntu Sever 22.04 LTS Jammy AMD ROCM Documentation AUTOMATIC1111 SDAI - Stable Diffusion Android client application Custom stable-diffusion-webui launch script ","date":"2024-06-01T00:00:00Z","image":"http://localhost:1313/post/how-to-run-stable-diffusion-in-vm-on-amd-gpu-automatic1111--kvm--gpu-passthrough/header_hu3aafa9004173c931941da8e2c440d12d_732838_120x120_fill_box_smart1_3.png","permalink":"http://localhost:1313/post/how-to-run-stable-diffusion-in-vm-on-amd-gpu-automatic1111--kvm--gpu-passthrough/","title":"How to run Stable Diffusion in VM on AMD GPU (AUTOMATIC1111 + KVM + GPU Passthrough)"},{"content":"Introduction to KVM over IP KVM over IP (Keyboard, Video, Mouse over IP) is a technology that allows remote access and control of a computer or server through a network connection. It enables users to manage the server\u0026rsquo;s or PC\u0026rsquo;s console as if they were physically present at the machine, regardless of their location.\nHow it works Hardware Device: KVM over IP typically involves a hardware device installed on the server or integrated into the server\u0026rsquo;s hardware. This device connects to the server\u0026rsquo;s keyboard, video, and mouse ports.\nNetwork Connection: The KVM over IP device is then connected to the network, allowing remote access to the server.\nAccess Software: Users can access the server\u0026rsquo;s console remotely using specialized software provided by the KVM over IP device manufacturer. This software allows users to view the server\u0026rsquo;s screen, control the keyboard and mouse input, and interact with the server as if they were physically present.\nSecurity Features: KVM over IP solutions often come with security features such as encryption and authentication to ensure secure remote access to the server.\nAdvantages over software remote desktop Low-Level Access: KVM over IP provides low-level access to the server\u0026rsquo;s console, allowing users to interact with the server or PC at the BIOS level and during the boot process. This level of access is not typically available with software remote desktop solutions, which operate within the operating system environment.\nOperating System Independence: Since KVM over IP operates at a hardware level, it is independent of the server\u0026rsquo;s operating system. This means it can be used to troubleshoot and manage servers even if the operating system is unresponsive or malfunctioning.\nOut-of-Band Management: KVM over IP provides out-of-band management capabilities, allowing administrators to remotely access and manage servers even if the network or operating system is down. This can be crucial for troubleshooting and maintenance tasks.\nPerformance: KVM over IP solutions often offer better performance compared to software remote desktop solutions, especially for tasks that require low-latency and high-resolution video output.\nNo Software Installation Required: Since KVM over IP is a hardware-based solution, it does not require any software installation on the server side, which can simplify deployment and maintenance.\nHardware solutions overview Commercial KVM switches There are plenty of KMV over IP devices available on the market, but they have some disatvantages in case you need some sort of solution for personal use, which includes this key points:\nCost: Commercial hardware KVM solutions typically involve higher upfront costs compared to open-source solutions, as they often come with additional features and support services.\nNon-free software: Most of the KVM switches that are manufactured for production use in server rooms and critical infrastructure datacenters are bundled with closed source software that has proprietary license.\nPiKVM The best solution for the personal home-lab server room or remote management of the PC is PiKVM - an open and inexpensive IP-KVM solution based on Raspberry Pi single board ARM computer.\nThe PiKVM device has many useful features to control remote machine, for example:\nStorage simulation: this allows you to attach local *.iso file to the remote machine, it allows you to reinstall any OS remotely.\nUSB removal/insertion simulation.\nATX power control.\nIf you want to have a PiKVM device for yourself there are 2 options:\nOrder a pre-built device on the official website.\nBuy one of the Raspberry Pi boards that are supported as well as some components and assemble the DIY device.\nBuilding own PiKVM v2 device I decided to build PiKVM v2 device myself, as there are no delivery of pre-built devices available for my country. Also DIY build gives more flexibility: you can build device according to needed features, you can choose whatever components you want.\nChoosing components After investigating the PiKVM official documentation it\u0026rsquo;s obvious that the best version to build is PiKVM v2. To assemble the device I used this list of hardware components:\n1 x Raspberry Pi 4 Model B with 2 Gb of RAM. 1 x Power adapter GAN 65W PD 1 x HDMI USB video capture card 1 x Kingston SD Card 32 Gb 10 class. 2 x USB-A to USB-C cables (to build cusom power cable). 1 X HDMI male-male cable. 1 x 80mm FAN (for active cooling). Building cusom power cable To be able to operate the remote machinve wiht our DIY KVM device computer should be able to recognize our device as HID USB device that will be treated by the computer just like another keyboard or mouse (the computer keyboard and mouse are also HID device).\nThe problem with Raspberry Pi 4 Model B board - it has only one USB-C port that is able to act as a HID device and that port is also used to power the device. To fix this there is workaround that requires tou build a special cable that can simultanously act as USB Device for the target host and receive external power from the power adapter.\nTo build this custom power cable you need 2 x USB-A male to USB-C male cables. I recommend choosing good quality cables because board requires 3 amp current power. The process of building cable includes this steps:\nTake the first cable and cut the isolation. Leave the data line wires (green and white) as is, and cut the power +5V (red) and ground (black) wires.\nTake the second cable and cut it completely. You need only the USB-A part of it.\nSolder power +5V (red) wire of the second USB-A cable part with the power +5V (red) wire of the USB-C cable part.\nSolder all of the 3 ground (black) wires of all the cable parts together.\nIsolate everything with the electric tape.\nSo the USB-C connector should be plugged in Raspberry Pi board, USB-A connector of the second cable (the one with power wired) should be plugged in power adapter, and the USB-A connector of the first cable (the one with data wires) should be plugged to the PC you want to control. For better understanding there is a cable soldering scheme below.\nAlso I recommend this video on YouTube from the PiKVM developer which explains the full process of making this custom cable.\nHere is the photo of cable I built. I labeled all the sides of the cable for myself to avoid mix up when connecting the hardware. The black USB-A cable should go to the power adapther, white USB-A cable should be connected to the computer, and the USB-C connector is for Raspberry Pi board.\nAssemble device in a custom case The components I use to build the device include external HDMI video signal capture card that should be connected to the specific USB port of the Raspberry Pi board. For my needs the device should be portable, so there is a risk of using the wrong port if I should connect the card every time. Considering this, the official Raspberry Pi case is not an option, also it costs too much in my opinion. So I decided to assamble the device in a single case for better portability and plug \u0026amp; play experience, every time I need to use it with another PC I simply need connect HDMI and USB cables to it.\nAlso it\u0026rsquo;s not a secret that Rasperry Pi boards can get overheated when working in extreme CPU load conditions or with 24/7 uptime. Taking that to account, I decided to construct a case in a way that all of the hardware will be cooled with 80mm fan.\nUnfortunately, I have no 3D printer to engineer and create a good quality case that will look like manufactured one. My case is fully hand made. I used mostly some old pieces of plastic panels that I had at the time.\nThe fan can be connected to the appropriate GPIO pins on the board. For that I used pin #2 for +5v (red wire) and pin #6 for ground (black wire) to power the fan.\nI used native connector that fan already had, but I had to place the pins on that connector in the correct order.\nAlso I decided to include the RJ-45 Ethernet port on the side, because the quality of stream is better and the latency is less when using cable internet connection instead of Wi-Fi. For that I made a custom cable extension and mounted the port near the HDMI input.\nFlashing PiKVM OS In order for Raspberry Pi to be able to act as a hardware KVM device the open source PiKVM OS should be flashed to the SD Card.\nThe correct image of the PiKVM OS that is suitable for particular Raspberry Pi model can be found on PiKVM Flashing OS page. In my case I downloaded DIY PiKVM V2 Platform, Raspberry Pi 4 for USB Dongle image. Because it matches the hardware I used (Raspblerry Pi 4 Model B and HDMI USB Video capture card).\nTo flash the OS image to the SD card there is the RPI Imager software availabe. As I am happy Arch Linux user (btw), I intalled it from official rpi-imager package:\nsudo pacman -S rpi-imager Then connect your SD card to the computer. As my computer does not have any built-in card reader I use micro-SD to USB-A adapter.\nTo flash the downloaded image follow this steps:\nOpen the RPI Imager. Press \u0026ldquo;Choose device\u0026rdquo; and select the model of your Raspberry Pi board. I selected \u0026ldquo;Raspberry Pi 4\u0026rdquo; here. Click \u0026ldquo;Choose OS\u0026rdquo;, scroll down to the bottom and select \u0026ldquo;Use custom image\u0026rdquo; option. Then in the file picker select the downloaded PiKVM image. Click \u0026ldquo;Choose storage\u0026rdquo; and select your SD-card. Be careful to select the correct device here because RPI imager will format the selected device. Ensure all fields are set correctly, then click next. You will see a modal dialog asking if you want to customize some settings, click \u0026ldquo;NO\u0026rdquo;. Finally confirm the device flash process and wait until RPI Imager finished flashing and validating the image. Configuring Wi-Fi connection (optional) As I mentioned earlier, it is always better to use Ethernet RJ-45 internet connection because it is more reliable and results in better video stream performance and lower latency. However as I want the device to be portable so I can take it with me and connect to another PC laptop, there might be no available Ethernet RJ-45 connection. For that purpose I set the Wi-Fi credentials of the \u0026ldquo;Mobile Hotspot\u0026rdquo; network of my Android phone that I always have with me, so when Raspberry Pi board boots without Ethernet cable connected, it will use its built-in Wi-Fi adapter and connect to my Android phone.\nTo setup Wi-Fi follow this steps:\nConnect the SD-card to the computer.\nYou will see that SD-card has more that one partition, you need to mount the first partition, which is called \u0026ldquo;PIBOOT\u0026rdquo;.\nLook for the file pikvm.txt in the root folder of the \u0026ldquo;PIBOOT\u0026rdquo; partition. If the file doest not exist - create it manually, in case file already exists don\u0026rsquo;t delete any configurations from it you need to add 2 parameters to the bottom of the file.\nAdd 2 parameters to the end of the file. Replace the credentials in the example to match your Wi-Fi network credentians.\n1 2 WIFI_ESSID=\u0026#39;my_wifi_network\u0026#39; WIFI_PASSWD=\u0026#39;the_most_secure_password_ever\u0026#39; Save the file. Then safely unmount the partition and eject the SD-card. Install the SD-card to the board. First boot It is important step, because during the first boot PiKVM OS initializes the necessary settings and generates the unique SSH keys and security certificates.\nFirst boot steps:\nInstall the SD-card with flashed PiKVM OS to the Raspberry Pi board.\nConnect the Raspbery Pi board to your Wi-Fi router using Ethernet RJ-45 cable. If you don\u0026rsquo;t have ability to use Ethernet during the first boot, you can configure Wi-Fi credentials (as shown in previous step), but before booting make sure that the Wi-Fi network is available.\nBoot the Raspberry Pi board. To do this connect the 5V 3A power source to the USB-C port of the board.\nWait some time until the PiKVM OS finishes the first boot initialization. This process may take up to 10 minutes. ðŸ”´ IMPORTANT: do not unplug the power cable from Rasperry Pi board until the first boot initialization is finished.\nAfter first boot process is finished, PiKVM device will connect to the network and get Local IP address in the Local network of your router. To detect what IP address was obtained by the PiKVM you can go to your router admin settings page, and look for the connected devices. In my case board obtained IP address 192.168.0.222, so I will use it in the all follwing exampels. In your case IP-address will be different.\nThen you need a computer or smartphone that is connected to the same Local network as PiKVM device. Open the web browser and navigate to the IP-addres url https://192.168.0.222. The default credentials are login admin with password admin.\nChange the default passwords to keep your device secure. There is a detailed description on how to do this in the PiKVM documentation.\nUpdating the PiKVM OS software PiKVM OS is open source operating system based on Arch Linux. It is important to egularly update your PiKVM OS to receive security updates and new features. Like on any Linux system update is done via the terminal, you can use either ssh shell or the Terminal in PiKVM web browser interface.\nTo update the OS do the following:\nOpen the terminal in the PiKVM web interface. Alternatively you can use ssh connection.\nObtain the root access by typing su - command and entering the root password.\nRun the pikvm-update command and wait until process finished. Please make sure your PiKVM device does not lose power and internet connection during the update process.\nSetting up Tailscale VPN (optional) There can be some edge case situations with not being able to have the access to the same Local network with PiKVM board, for example when I have PiKVM connected to my home server, and I am physically not at home and I need remote access to my server.\nAlso considering that I want my PiKVM device to be as portable as possible, I also want to be able to borrow my PiKVM device to someone, so that person can take it and connect to their home PC and local network, and I can connect to that PC remotely and help reinstalling OS for example.\nThe Tailscale VPN is a free tool (for the personal use) that can be used as a solution to the cases I described above. It allows to access PiKVM from the Internet. To setup it follow this steps:\nOpen the terminal in the PiKVM web interface. Alternatively you can use ssh connection.\nInstall tailscale systemd service by entering this command sequence to the terminal:\n1 2 3 4 5 su - rw pacman -S tailscale-pikvm systemctl enable --now tailscaled tailscale up The terminal will show a link that you need to copy and follow in your web browser. After following the link login or sign up to Tailscale VPN, and finally your PiKVM device will be attached to that account.\nInstall the Tailscale Client to the system you want to use (not the PC you want to control over the PiKVM) and connect it to the VPN. To do this follow the instructions here.\nAfter setting up client on your system visit Tailscale Admin Page. If you did everything correctly you should see your PiKVM device and the system that is used for the remote connection to PiKVM.\nIn the screenshot above I highlighted the example of IP address that should be used in the browser to connect to the PiKVM remotely.\nMake sure you disable expiration for the PiKVM device. To do this clich \u0026ldquo;three dots\u0026rdquo; icon on the right side of your PiKVM device in list and select the \u0026ldquo;Disable key expiry\u0026rdquo; option. After setup is complete, every time the PiKVM device is booted it will be automatically connected to the VPN network, so if I need remote access to it from anywhere, I can just connect my laptop to the same VPN and manage my PiKVM device.\nHow to use PiKVM device to control PC / Laptop / Server This is the istruction for using PiKVM board as a portable device. To assemble the connection you need to have: PiKVM device, machine you want to control, custom power cable and adapter, Ethernet RJ-45 cable, HDMI cable.\nTake the Ethernet RJ-45 cable and connect it to the PiKVM device and your Wi-Fi router. Take the HDMI male to male cable and connect it to the PiKVM device and the machine you want to control. Take the custom power cable and connect: USB-C connector to the PiKVM device. USB-A connector labeled \u0026ldquo;PC/Laptop\u0026rdquo; to the machine you want to control. USB-A connector labeled \u0026ldquo;Power\u0026rdquo; to the power adapter. Plug the power adapter to the electric outlet. The PiKVM device should boot and connect to the network.\nPower up the machine you want to control.\nAfter that you should be able to connect to the PiKVM device and control your machine.\nConclusion As a result of this project the portable KVM over IP device was assembled that has many useful features:\nDevice can use both Ethernet and Wi-Fi connection. Device can be fully managed remotely using VPN. Device assembled in a custom unique hand made case and uses active cooling that prevents the device from overheat so it can be safely used 24/7. ","date":"2024-03-29T00:00:00Z","image":"http://localhost:1313/post/assembling-a-pikvm-v2-device-for-remote-kvm-over-ip-control-of-a-computer-or-server/header_huaf7132429931ddcde60d8d9ee0a89d27_286083_120x120_fill_q75_box_smart1.jpg","permalink":"http://localhost:1313/post/assembling-a-pikvm-v2-device-for-remote-kvm-over-ip-control-of-a-computer-or-server/","title":"Assembling a PiKVM v2 device for remote KVM over IP control of a computer or server"},{"content":"What is Linux Zen Kernel ? The Linux Zen Kernel is a customized version of the Linux kernel that focuses on providing improved performance, responsiveness, and flexibility for desktop and workstation users. It is developed and maintained by the Zen Kernel project, which aims to optimize the kernel for desktop use cases.\nThe Zen Kernel incorporates various patches and optimizations aimed at reducing latency, improving system responsiveness, and enhancing overall system performance. These optimizations may include tweaks to the scheduler, I/O scheduler improvements, CPU scheduling enhancements, and other performance-related adjustments.\nUsers who prioritize desktop or workstation performance and responsiveness may choose to use the Linux Zen Kernel over the standard Linux kernel provided by their distribution. However, it\u0026rsquo;s important to note that the Zen Kernel is a third-party modification and may not be officially supported by all Linux distributions. Users interested in trying out the Zen Kernel should review documentation and instructions provided by the Zen Kernel project or their specific distribution\u0026rsquo;s community.\nKey benefits Some of the key features and optimizations typically found in the Linux Zen kernel include:\nLow-Latency Patchset: These patches aim to reduce the latency of the kernel, improving responsiveness, which is particularly important for interactive desktop use, audio processing, and gaming.\nBFQ I/O Scheduler: The Zen kernel often includes the BFQ (Budget Fair Queueing) I/O scheduler, which prioritizes I/O requests based on the process generating them, aiming to provide smoother system performance, especially during multitasking or when dealing with interactive applications.\nAdditional CPU Schedulers: In some cases, Zen kernel includes alternative CPU scheduling algorithms or optimizations to improve multitasking performance and responsiveness.\nPreemption and Real-Time Support: The kernel may include patches to enhance preemption and real-time capabilities, reducing delays and improving responsiveness for time-sensitive tasks.\nTuned Defaults: Some kernel parameters are tuned by default to better suit desktop and multimedia workloads.\nStability and Reliability: While emphasizing performance improvements, Zen kernel developers typically ensure that stability and reliability are maintained, though users should always be aware that any kernel modifications may introduce risks.\nInstallation (Arch Linux) Linux Zen Kernel is included in Arch Linux pacman package manager and can be easily installed using pacman:\nsudo pacman -S linux-zen linux-zen-headers After that kernel is actually installed on your system, but now it\u0026rsquo;s time to tell your bootloader to boot with Linux Zen Kernel instead of the default one. This can depend on your bootloader, in my case I am using systemd-boot as my bootloader.\nThe boot entries are located at /boot/loader/entries directory, so navigate there:\ncd /boot/loader/entries/ And list all your boot entries:\nls -la In my case I have only one entry arch.conf:\n1 2 3 drwxr-xr-x 2 root root 4096 Feb 24 14:42 . drwxr-xr-x 3 root root 4096 Mar 25 20:15 .. -rwxr-xr-x 1 root root 208 Mar 18 18:31 arch.conf I recommend to leave the original boot entry and original Linux Kernel for the backup, so if new kernel fails to boot you will always have an option to boot from the stock kernel in the bootloader.\nCreate a duplicate of the original boot entry file:\nsudo cp arch.conf arch-zen.conf Then edit the arch-zen.conf file using your text editor of choice, in my case I use nano like this:\nsudo nano arch-zen.conf Then modify the config and replace 3 params (title, linux, initrd) according to the example above:\nBefore (original arch.conf file):\n1 2 3 title Arch Linux linux /vmlinuz-linux initrd /initramfs-linux.img After (modify in arch-zen.conf file):\n1 2 3 title Arch Linux Zen linux /vmlinuz-linux-zen initrd /initramfs-linux-zen.img After saving the file reboot your system, and select the Arch Linux Zen during boot.\nConclusion To sum up, the Linux Zen kernel is tailored for users who prioritize desktop responsiveness, multimedia performance, and gaming experience. However, users should carefully evaluate whether the specific optimizations and patches provided by the Zen kernel align with their needs and preferences, as some features may trade off general-purpose performance or compatibility for specialized use cases.\n","date":"2024-03-25T00:00:00Z","image":"http://localhost:1313/post/install-linux-zen-kernel-on-arch-linux-to-improve-performance/header_hu4b29e357e57d8fe0ebc72c8e749e5da1_83937_120x120_fill_box_smart1_3.png","permalink":"http://localhost:1313/post/install-linux-zen-kernel-on-arch-linux-to-improve-performance/","title":"Install Linux ZEN kernel on Arch Linux to improve performance"},{"content":"Introduction GPU passthrough in KVM (Kernel-based Virtual Machine) refers to the process of directly assigning a physical GPU (Graphics Processing Unit) to a virtual machine running on a KVM hypervisor. This allows the virtual machine to have exclusive access to the GPU, essentially bypassing the hypervisor\u0026rsquo;s virtualization layer.\nFor many people this can sound complicated, but with Linux and KVM it\u0026rsquo;s actually easy and setup includes this 5 steps:\nIdentifying the GPU. The first step is to ensure that the GPU you want to passthrough is compatible with your hardware and supports the necessary features for passthrough. This often requires VT-d (Intel Virtualization Technology for Directed I/O) or AMD-Vi (AMD Virtualization I/O) support on the CPU, as well as an IOMMU (Input-Output Memory Management Unit) on the motherboard.\nConfiguring the Host. You\u0026rsquo;ll need to configure the host system to enable IOMMU, VT-d, or AMD-Vi in the BIOS/UEFI settings, as well as load necessary kernel modules and drivers.\nIsolating the GPU. The GPU intended for passthrough needs to be isolated from the host operating system so that it can be exclusively assigned to the virtual machine. This is typically done by blacklisting the GPU driver from loading on the host.\nConfiguring the Virtual Machine. Within the KVM setup, you configure the virtual machine to use the passthrough GPU. This involves specifying the GPU as a PCI device to be passed through to the VM.\nDriver Installation. Once the GPU is passed through to the virtual machine, you install the necessary drivers within the guest operating system just as you would on a physical machine.\nHardware requirements It is important to mention that this kind of setup requires your computer hardware to meet some requirements.\nCPU Your CPU must support hardware virtualization extensions such as Intel VT-x (Intel Virtualization Technology) or AMD-V (AMD Virtualization). Additionally, for GPU passthrough, you\u0026rsquo;ll need support for Intel VT-d (Intel Virtualization Technology for Directed I/O) or AMD-Vi (AMD Virtualization I/O) extensions, which allow direct access to I/O devices from virtual machines.\nMotherboard Your motherboard must have an IOMMU (Input-Output Memory Management Unit) that supports PCIe device isolation and passthrough. Most modern motherboards have this feature, but you should check your motherboard\u0026rsquo;s specifications to ensure compatibility.\nGPU The GPU you must support UEFI. Ideal choice is a GPU that supports the necessary features for passthrough, such as AMD\u0026rsquo;s Radeon Pro or Nvidia\u0026rsquo;s Quadro series GPUs, as consumer-grade GPUs may have limitations or require workarounds.\nAlso if you want the ability to use your host Linux OS at the same time with the guest Windows OS you need to have 2 GPU (one for host, another for guest) and it is recommended to have 2 monitors.\nIf your CPU has built-in graphics unit and your motherboard has video output, it may be fine to have only 1 external PCI GPU. In that case the PCI GPU is passed to the VM, and Linux host uses CPU graphics for output.\nGPU Passthrough setup Review your hardware The very first step for the setup is to review the hardware you have. If you want achieve the same setup as in my example make sure you have 2 GPUs (at least one should be external PCI GPU).\nMy hardware specifications are:\nMotherboard: MSI MPG z490 Gaming Edge WiFi CPU: Intel Core i9-10850K GPU 1: AMD Radeon RX 6750XT (for Linux HOST) GPU 2: AMD Radeon RX 6600 (for Windows GUEST) Monitors: 2 x FullHD 1920x1080 @ 75 Hz Install and configure KVM This guide is written assuming you already have a working Linux system with installed and configured KVM (libvirt) and the software to manage your VMs (virt-manager, virt-viewer).\nIf you don\u0026rsquo;t have KVM installed yet, see my another guide: How to install KVM on Arch Linux.\nModify motherboard BIOS settings In order for KVM to work on your system Virtualization Technology should be enabled. It may be named differently depending on your motherboard/CPU manufacturer, but usually it is VT-D for Intel and AMD-Vi for AMD respectively.\nIf you have a setup with the integrated CPU graphics you also will need to change the graphics initialization priority. It is needed to prevent your external GPU initialization on BIOS POST startup, and delegate a video signal to integrated GPU instead. In my case I don\u0026rsquo;t need this step, as I have 2 external PCI GPUs.\nEnable IOMMU IOMMU is a generic name for Intel VT-d and AMD-Vi.\nTo enable IOMMU in the Linux system you should pass the correct boot kernel parameter. Adding the kernel boot parameters may be different, depending on what bootloader you are using.\nIf are using:\nIntel CPU - add flags: intel_iommu=on iommu=pt AMD CPU - add flags: iommu=pt In AMD case kernel detects if IOMMU should be on from BIOS, and iommu=pt prevents Linux from processing devices that can\u0026rsquo;t be passed through.\nOption 1: GRUB If you are using grub as your bootloader, you can modify your kernel parameters in file /etc/default/grub and append the flags to GRUB_CMDLINE_LINUX parameters, for example:\n1 2 3 ... GRUB_CMDLINE_LINUX=\u0026#34;quiet splash intel_iommu=on iommu=pt\u0026#34; ... Then save the file and re-generate your GRUB config with the command:\nsudo grub-mkconfig -o /boot/grub/grub.cfg Option 2: Systemd Boot If you are using the systemd boot bootloader, you need to edit your boot entry file. In my case this file is located at /boot/loader/entries/arch.conf.\nKernel parameters should be appended to the end of options parameter like this:\n1 2 3 ... options rw quiet splash intel_iommu=on iommu=pt ... After you added the correct kernel parameters to your bootloader, reboot your computer.\nReviev your IOMMU groups An IOMMU (Input-Output Memory Management Unit) group is a logical grouping of devices that are controlled by the same IOMMU. The primary purpose of an IOMMU group is to facilitate device isolation and management in virtualized environments, particularly when using technologies like GPU passthrough.\nIt is important that you have only devices related to the GPU your intend to pass through in the same IOMMU group. Because if you have something else, it should be also passed to the VM or otherwise passthrough will not work.\nYou can use the following script to verify your iommu groups:\n1 2 3 4 5 6 7 8 #!/bin/bash shopt -s nullglob for g in $(find /sys/kernel/iommu_groups/* -maxdepth 0 -type d | sort -V); do echo \u0026#34;IOMMU Group ${g##*/}:\u0026#34; for d in $g/devices/*; do echo -e \u0026#34;\\t$(lspci -nns ${d##*/})\u0026#34; done; done; You need to save the script contents to some file (for example iommu.sh) and make this file executable (chmod +x iommu.sh), then you can just run it using ./iommu.sh.\nIn my case IOMMU groups are:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 IOMMU Group 0: 00:00.0 Host bridge [0600]: Intel Corporation Comet Lake-S 6c Host Bridge/DRAM Controller [8086:9b33] (rev 05) IOMMU Group 1: 00:01.0 PCI bridge [0604]: Intel Corporation 6th-10th Gen Core Processor PCIe Controller (x16) [8086:1901] (rev 05) 01:00.0 PCI bridge [0604]: Advanced Micro Devices, Inc. [AMD/ATI] Navi 10 XL Upstream Port of PCI Express Switch [1002:1478] (rev c0) 02:00.0 PCI bridge [0604]: Advanced Micro Devices, Inc. [AMD/ATI] Navi 10 XL Downstream Port of PCI Express Switch [1002:1479] 03:00.0 VGA compatible controller [0300]: Advanced Micro Devices, Inc. [AMD/ATI] Navi 22 [Radeon RX 6700/6700 XT/6750 XT / 6800M/6850M XT] [1002:73df] (rev c0) 03:00.1 Audio device [0403]: Advanced Micro Devices, Inc. [AMD/ATI] Navi 21/23 HDMI/DP Audio Controller [1002:ab28] ... IOMMU Group 18: 07:00.0 VGA compatible controller [0300]: Advanced Micro Devices, Inc. [AMD/ATI] Navi 23 [Radeon RX 6600/6600 XT/6600M] [1002:73ff] (rev c7) IOMMU Group 19: 07:00.1 Audio device [0403]: Advanced Micro Devices, Inc. [AMD/ATI] Navi 21/23 HDMI/DP Audio Controller [1002:ab28] ... Ensure that the GPU and its associated devices are not grouped with other critical system devices (e.g., USB controllers, Ethernet controllers) that you do not intend to pass through to the virtual machine. Having the GPU in its own isolated IOMMU group is crucial for reliable passthrough.\nIn this case the AMD RX 6600 GPU that I want to pass to VM has 2 devices 07:00.0 and 07:00.1, each in separate IOMMU groups 18 and 19, and no other devices are in groups 18 and 19. Than means I can passthrough it with no problem.\nIf we look at the devices for AMD RX 6750 XT which are 03:00.0 and 03:00.1 in IOMMU group 1, there are also some PCI birdge devices, which in theory does not prevent passthrough this GPU as well as long as you also pass this PCI bridges to VM. The real problem would be if for example IOMMU group 1 contained some brand another PCI device (like USB bridge or sound card).\nIn case you have more devices than you need in IOMMU group with the GPU you could try:\nIf your motherboard has many PCI Express slots, try to install GPU in different PCIE slots and see if it helps. In case you can not find optimal IOMMU group with no extra devices, you can patch your Linux kernel with ACS Override patch (but it is out of scope for this article). Isolate your GPU Isolating the GPU refers to the process of ensuring that the GPU is exclusively available for use by a specific virtual machine, typically through binding it to the VFIO (Virtual Function I/O) driver. This process involves several steps to prevent the GPU from being used by the host operating system or other virtual machines:\nUnbinding from Host Driver. Initially, the GPU is typically bound to a driver used by the host operating system for normal graphics operations. To isolate the GPU, this binding needs to be undone so that the GPU can be claimed by the VFIO driver. This step effectively detaches the GPU from the host OS.\nBinding to VFIO Driver. Once unbound from the host driver, the GPU is then bound to the VFIO driver, which allows it to be passed through to the virtual machine. The VFIO driver provides the necessary functionality for direct device assignment (DDA) in virtualized environments, ensuring that the GPU is accessible to the virtual machine.\nIsolating means that after you boot your host Linux system you can not use the isolated GPU (which is bound to vfio driver), you can only use it to pass to the VM.\nTo isolate the GPU you need to know the deice IDs of your GPU video and audio PCI output device. You can use iommu.sh script from previous step to get this information. In my case the device IDs are: 1002:73df and 1002:ab28.\nVFIO Early load To load vfio early and bind the necessary GPU to vfio create a config at /etc/modprobe.d/vfio.conf which in my case looks like this:\n1 2 softdep amdgpu pre: vfio-pci options vfio-pci ids=1002:73ff,1002:ab28 disable_vga=1 kvm.ignore_msrs=1 Here the line softdep amdgpu pre: vfio-pci allows vfio-pci driver to be forced to load before the amdgpu driver, I need this because I have 2 external PCI GPUs of the same manufacturer, and this allows to bind RX 6600 GPU to vfio-pci driver and then bind RX 6750 XT to the amdgpu driver.\nIn your case if you have NVidia gpu then vfio-pci should be loaded before the nvidia (proprietary) or before the nouveau (free) driver.\nInitramfs configuration In my case I use mkinitcpio as initramfs builder. To configure mkinitcpio edit the /etc/mkinitcpio.conf file.\nAdd the necessary modules to the MODULES array:\n1 2 3 ... MODULES=(vfio_pci vfio vfio_iommu_type1) ... Then ensure you have modconf hook defined in the HOOKS array:\n1 2 3 ... HOOKS=(... modconf ...) ... Save the file, and then execute a command for initramfs regeneration:\nsudo mkinitcpio -P After that, you need to reboot the computer to apply changes.\nSetup a new VM and install Windows 10/11 Now it is a time to setup a new VM and install a new Windows system. I recommend setup and install Windows without any passed PCI devices, and add them to the VM later after installation is finished.\nAll the setup is easier to do using the virt-manager graphical application.\nOpen the virt-manager application.\nClick \u0026ldquo;New Virtual Machine\u0026rdquo;.\nSelect Local install media. Ususally it should be Windows 10/11 iso file which can be downloaded from microsoft website.\nBrowse to your Windows 10/11 iso. It is recommended that your iso to be located somewhere in your home user folder to avouid file permission problems. Also it is important to make sure to identify the OS as \u0026ldquo;Windows 10\u0026rdquo; or \u0026ldquo;Windows 11\u0026rdquo; at the bottom field. Allocate CPU and Memory. Be sure to reserve at least 2 CPU cores and some RAM for your linux host system to handle networking and some other stuff. In my case I allocated 2 CPU cores and 8 Gb of RAM to the VM, you can allocate more if you want. Enable storage for this virtual machine and Create a disk image. I recommend to give at least 50 Gb for the Windows VM. Then DO NOT CLICK FINISH, and do the following:\nEnsure Customize Configuration before install is checked. Give your VM a name. Under network selection, select Host device [your device ID] -\u0026gt; Source Mode -\u0026gt; Bridge or NAT). Now you can click Finish. Ensure to select the Q35 chipset in the \u0026ldquo;Overview\u0026rdquo; and the UEFI firmware. GPU Passthrough will not work on BIOS firmware, it requires to use the UEFI. Then click on \u0026ldquo;Begin Installation\u0026rdquo; and install Windows as usual, after you have the system installed proceed to the next step.\nAdjust specific VM settings There is a need to add some flags that allow to hide the fact that you are using VM from the GPU drivers installer.\nTo edit the VM settings make sure your VM is not running and type this command in terminal:\nsudo virsh edit win10 where you should replace win10 with your actual VM name.\nThen do the following XML file modifications in the editor:\nAdd \u0026lt;vendor_id state='on' value='randomid'/\u0026gt; to the \u0026lt;hyperv\u0026gt; section. Add \u0026lt;kvm\u0026gt;\u0026lt;hidden state='on'/\u0026gt;\u0026lt;/kvm\u0026gt; to the \u0026lt;features\u0026gt; section. As the result the structure should be like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 ... \u0026lt;features\u0026gt; ... \u0026lt;hyperv\u0026gt; ... \u0026lt;vendor_id state=\u0026#39;on\u0026#39; value=\u0026#39;randomid\u0026#39;/\u0026gt; ... \u0026lt;/hyperv\u0026gt; \u0026lt;kvm\u0026gt; \u0026lt;hidden state=\u0026#39;on\u0026#39;/\u0026gt; \u0026lt;/kvm\u0026gt; ... \u0026lt;/features\u0026gt; ... Passthrough GPU to the VM In the virt-manager applcation open your VM settings and navigate to your VM settings and here need to add 2 PCI host devices related to the GPU you intend to passthrough.\nClick \u0026ldquo;Add PCI Passthrough Devices\u0026rdquo;.\nClick \u0026ldquo;Add Hardware\u0026rdquo;.\nSelect PCI Host Device\nScroll down until you find the GPU you want to pass through. The ID\u0026rsquo;s should match your outputs from earlier. For example, in my case:\n0000:07:00:0 Advanced Micro Devices, Inc. [AMD/ATI] Navi 23 [Radeon RX 6600/6600 XT/6600M] 0000:07:00:1 Advanced Micro Devices, Inc. [AMD/ATI] Navi 21/23 HDMI/DP Audio Controller [1002:ab28] You need to do this twice for the GPU\u0026rsquo;s video and audio device respectively.\nThen if you have a second monitor try to plug it to the video output of the GPU you passed to the VM and boot the VM, if everything is set up correctly you should see the VM video output on the separate monitor.\nInstall GPU drivers After you successfully booted your Windows VM with external GPU working it\u0026rsquo;s time to download and install latest drivers from your GPU manufacturer.\nOfficial Nvidia GPU drivers Official AMD GPU drivers Conclusion Configured Windows VM with GPU passthrough can handle specific graphical apps (like gaming or video editing) with a near native performance, and as a result allows to use your host Linux system and a Windows guest system at the same time.\nBenefits of GPU passthrough in KVM include:\nImproved Performance: By directly accessing the physical GPU, the virtual machine can achieve near-native performance for graphics-intensive tasks such as gaming, video editing, and 3D rendering.\nReduced Overhead: Since the virtual machine bypasses the hypervisor\u0026rsquo;s virtualization layer for GPU operations, there\u0026rsquo;s less overhead compared to traditional GPU virtualization techniques like GPU emulation or virtual GPU (vGPU) solutions.\nSupport for GPU-accelerated Workloads: GPU passthrough enables virtual machines to leverage hardware acceleration for tasks that rely on GPU processing power, leading to faster execution times and improved efficiency.\nCompatibility with GPU-specific Software: Certain applications and workflows require direct access to a physical GPU, which may not be possible with virtual GPU solutions. GPU passthrough allows such software to run seamlessly within a virtualized environment.\nEnhanced Gaming Experience: Gamers can benefit from GPU passthrough by running games within a virtual machine with minimal performance degradation, enabling them to utilize hardware-accelerated graphics without dual-booting or dedicating a separate physical machine for gaming.\n","date":"2024-03-18T00:00:00Z","image":"http://localhost:1313/post/gpu-pci-passthrough-to-windows-kvm-on-arch-linux/header_hued49b8fc40a4ce925bd267fd9ac65523_97296_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"http://localhost:1313/post/gpu-pci-passthrough-to-windows-kvm-on-arch-linux/","title":"GPU PCI passthrough to Windows KVM on Arch Linux"},{"content":"Introduction KVM, which stands for Kernel-based Virtual Machine, is a virtualization solution for Linux operating systems. It allows you to run multiple virtual machines (VMs) on a single physical machine by leveraging hardware virtualization features built into modern processors.\nHere\u0026rsquo;s a breakdown of what KVM offers:\nHypervisor: KVM acts as a hypervisor, which is a piece of software that creates and runs virtual machines. It utilizes the virtualization extensions present in modern processors (such as Intel VT-x or AMD-V) to provide hardware-assisted virtualization.\nKernel Integration: KVM is integrated into the Linux kernel, which means it leverages the kernel\u0026rsquo;s functionality and benefits from ongoing kernel improvements. This integration provides better performance and stability for virtualized environments.\nFull Virtualization: KVM supports full virtualization, which allows guest operating systems to run unmodified. This means you can run a variety of operating systems, including Linux, Windows, and others, as virtual machines on a KVM-enabled host.\nPerformance: KVM is known for its high performance, thanks to its hardware-assisted virtualization support and tight integration with the Linux kernel. This allows for efficient resource utilization and minimal overhead when running virtualized workloads.\nManagement Tools: KVM can be managed through various tools, including command-line utilities like virsh and graphical interfaces like Virt-Manager. These tools provide administrators with the ability to create, configure, and manage virtual machines on KVM hosts.\nOverall, KVM is a powerful and versatile virtualization solution for Linux-based systems, offering performance, flexibility, and ease of management for virtualized environments.\nInstallation Check Virtualization Support Before installing KVM, ensure that your processor supports virtualization and it is enabled in the BIOS settings. Most modern processors support virtualization, but it\u0026rsquo;s good to double-check.\nFor detailed information about checking your hardware please refer to the Arch Wiki - Cheking support for KVM.\nInstall Required Packages Open a terminal and install the necessary packages. This includes the QEMU disk image utility qemu, the KVM kernel module kvm, and the libvirt virtualization API and management tool libvirt.\nsudo pacman -S virt-manager virt-viewer qemu dnsmasq bridge-utils Configure libvirt Service Libvirt is a toolkit to interact with virtualization capabilities of the Linux kernel. Enable the libvirt service to manage virtual machines.\nsudo systemctl enable --now libvirtd.service Enable autostart for the default NAT virtual network for your VMs:\nsudo virsh net-start default sudo virsh net-autostart default Then edit the libvirt config at /etc/libvirt/libvirtd.conf, and set the parameters:\n1 2 unix_sock_group = \u0026#34;libvirt\u0026#34; unix_sock_rw_perms = \u0026#34;0770\u0026#34; To actually make configurtion above work add your current user to the libvirt group:\nsudo usermod -a -G libvirt $(whoami) newgrp libvirt Then finally restart the libvirt daemon to apply the changes:\nsudo systemctl restart libvirtd.service After that you should be able to launch virt-manager and use KVM virtualization, but if you see some errors try rebooting your machine, if this does not help review your libvirt daemon logs and configuration.\nEnable nested virtualization (optional) Nested virtualization enables existing virtual machines to be run on third-party hypervisors and on other clouds without any modifications to the original virtual machines or their networking.\nTo enable it (non-permanently) use the following terminal commands:\nsudo modprobe -r kvm_intel sudo modprobe kvm_intel nested=1 Then to verify that it is enabled, check the output of the command:\ncat /sys/module/kvm_intel/parameters/nested it should print Y as output if Nested virtualization is enabled.\nTo make this change permanently applied when your machine is booted use command:\necho \u0026#34;options kvm-intel nested=1\u0026#34; | sudo tee /etc/modprobe.d/kvm-intel.conf Reference Arch Wiki - KVM. Wikipedia - KVM. Kernel Virtual machine source code. ","date":"2024-03-15T00:00:00Z","image":"http://localhost:1313/post/how-to-install-kvm-on-arch-linux/header_hu9582e9719e96abf49c172162d020cea7_78147_120x120_fill_box_smart1_3.png","permalink":"http://localhost:1313/post/how-to-install-kvm-on-arch-linux/","title":"How to install KVM on Arch Linux"},{"content":"Introduction SSH key-based authentication is a method of securely logging into a remote system or server using cryptographic keys instead of passwords. It works based on public-key cryptography, where a pair of keys is generatedâ€”a public key and a private key. The public key is stored on the server, and the private key is kept secure on the client side.\nHow it works? Key Generation: The user generates a pair of cryptographic keysâ€”a public key and a private key. The private key is kept securely on the user\u0026rsquo;s computer, while the public key is placed on the server.\nServer Configuration: The public key is added to the server\u0026rsquo;s list of authorized keys. This means the server will recognize the corresponding private key when presented during login attempts.\nLogin Attempt: When the user attempts to log in to the server, the client software presents the private key. The server checks if this private key corresponds to any of the public keys it has on record for authorized users.\nAuthentication: If the server finds a match, it allows the user to log in without the need for a password.\nWhy using keys is more secure? Key-based authentication is considered more secure than password-based authentication for several reasons:\nResistance to brute-force attacks: Passwords can be guessed or cracked through brute-force attacks. However, with key-based authentication, an attacker would need to possess the private key, which ideally should be securely stored and not easily accessible.\nNo transmission of passwords: In password-based authentication, the password is transmitted over the network, which can potentially be intercepted by attackers. With key-based authentication, the private key never leaves the client machine, so there\u0026rsquo;s no risk of interception.\nStronger encryption: SSH keys use strong cryptographic algorithms for authentication, making them resistant to various types of attacks.\nNo reliance on human-generated passwords: Human-generated passwords can be weak and prone to being compromised. Keys, on the other hand, are randomly generated and typically much longer, making them harder to guess.\nOverall, SSH key-based authentication offers a higher level of security and is recommended for remote access to servers and systems, especially in environments where security is paramount.\nImplementation example Next let\u0026rsquo;s talk how actually implement RSA key based authorizatoin to access the remote server via SSH protocol. This guide assumes you have local and remote (server) Linux systems, and you already have access to your server via password based ssh protocol.\nGenerate the new ssh key ðŸŸ¢ Security Tip: if you have multiple servers that you want to access via key based SSH it is strongly recommended to create different ssh key for every server.\nAll the ssh keys are stored in the .ssh directory that is located at your user home directory, so we need to change the working dir in the terminal:\ncd ~/.ssh In my case there was no .ssh directory, so I created it manually:\nmkdir ~/.ssh Then to generate a new key type this:\nssh-keygen -t rsa -b 4096 The parameters in the generation command above mean:\n-t rsa: Specifies the type of key to create. In this case, it indicates that the RSA algorithm should be used. RSA (Rivest-Shamir-Adleman) is a widely used public-key cryptosystem for secure data transmission.\nb 4096: Specifies the number of bits in the key. In this case, it sets the key size to 4096 bits. Larger key sizes generally provide stronger security but may also require more computational resources for encryption and decryption.\nThe key creation wizard will ask you some parameters:\nEnter a file in which save a key. In case you have multiple keys for multimple servers it is recommended to name your key files in a way you can later understand what key is used to access some particular server.\nEnter key passphraze (twice). To actually unlock the ssh key you need to specify a secure password. It means that in case your key becomes stolen or compromised it will be still protected by password.\nðŸŸ¢ Security Tip: it is recommended to have different password for the ssh key and for the remote unix user.\nFinally, after key is created you will see the output in terminal like this:\nCopy public key to your server There are many ways you can copy the public ssh key to your server, but the most secure way is to use SCP (secure copy).\nðŸŸ¡ Note: scp and ssh commands require to enter the password of the remote user.\nTo copy the public key use a command like this, but don\u0026rsquo;t forget to replace the parameters with your actual ones:\nscp dc1srv1.pub user@192.168.0.2: The parameters of the scp command mean:\ndc1srv1.pub is a public key file that was created at previous step. user is the username on the remote machine. 192.168.0.2 - the IP or domain of the remote machine. : - the path where to copy dc1srv1.pub file (in this case home directory of the remote user). Allow the public key on sever To do this we need to connect to a server shell via SSH protocol, in another terminal, connect to the remote shell:\nssh user@192.168.0.2 Then check that you also have .ssh directory in the home folder of the remote user, if you don\u0026rsquo;t, create it:\nmkdir ~/.ssh Create an authorized_keys file in .ssh directory:\ntouch ~/.ssh/authorized_keys Copy the contents of the public key to the authorized_keys file:\ncat ~/dc1srv1.pub \u0026gt;\u0026gt; ~/.ssh/authorized_keys After that the transferred at the previous step public key file is no longer needed, so it can be deleted:\nrm ~/dc1srv1.pub Change SSH configuration To change the ssh configuration edit the file /etc/ssh/sshd_config:\nsudo nano /etc/ssh/sshd_config In this file you should find some paramters, and set their values explicitly:\n1 2 3 4 5 6 7 8 9 10 11 12 ... # Disallow connection as root user PermitRootLogin no # Enable public key authentication PubkeyAuthentication yes # Disable challenge response ChallengeResponseAuthentication no # Enable PAM UsePAM yes # Disable password based authentication PasswordAuthentication no ... After saving the file, restart the sshd service:\nsudo systemctl restart sshd ðŸ”´ IMPORTANT: After the sshd restart the new configuration will take effect. It is recommended to NOT CLOSE your currently opened ssh session and keep it until you test and ensure your new ssh key works in ANOTHER terminal window. In this case if something goes wrong you can still revert the changes back to ssh_config file and restart the sshd daemon to apply previous configuration.\nCheck the SSH authentication Verify password based authentication is not allowed First, let\u0026rsquo;s verify that we can not connect as root and as a generic user, so for the command:\nssh root@192.168.0.2 or:\nssh user@192.168.0.2 You should get an error Permission denied (publickey).\nVerify key based authentication works The ssh command to connect to your remote server will be a bit different, as you should specify the ssh key that needs to be used for authentication:\nssh -i ~/.ssh/dc1srv1 user@192.168.0.2 After typing this command you will be promted to enter the password that was used during ssh key generation.\nConclution After you complete this tutorial your server is using secure key based ssh authentication. Keep in mind that you need to backup your ssh keys, because if you lose your ssh key there is no way you can connect to the remote machine via the ssh (in this case only bare metal access to the remote machine can help reset a key, or if you are using some VPS/hosting provider some admin web panels allow resetting ssh settings).\n","date":"2024-03-10T00:00:00Z","image":"http://localhost:1313/post/configure-ssh-authorization-based-on-rsa-key/header_hu7e01c4855c80de0ee2ab35b4b9e2ae18_52924_120x120_fill_q75_box_smart1.jpg","permalink":"http://localhost:1313/post/configure-ssh-authorization-based-on-rsa-key/","title":"Configure SSH authorization based on RSA key"},{"content":"Introduction This is my own guide for installing Arch Linux on bare metal machine with UEFI, encrypted LVM and separate /home partition.\nSteps First you need to create and boot the installation media on your PC, as the result you will boot into plain console.\nIncrease font size As most modern laptops/PCs have large resolution displays I recommend increasing the font size:\nsetfont ter-132b Setup the internet connection In this example I have a laptop with Wi-Fi modem, so I\u0026rsquo;ll be using iwd to setup the internet connection.\nRun iwd:\niwctl View a list of Wi-Fi adapters:\ndevice list Usually you should see one Wi-Fi device in output, in my case it is wlan0\nThen if you know the station SSID and password, simply connect to the station, and don\u0026rsquo;t forget to replace {SSID} with your actual value:\nstation wlan0 connect {SSID} Then exit the iwctl by typing exit, then do the ping 8.8.8.8 to ensure you are connected to the internet.\nSynchronize system clock timedatectl set-ntp true Partition your disk In my case I want to have separate root, /boot and /home partitions, moreover / and /home should be encrypted by LVM and be in the same volume group.\nDetect your drive First, we need to know what device to user, to view the disk devices use:\nfdisk -l In my case it is the NVMe SSD drive /dev/nvme0n1.\nPartitioning Next, use gdisk /dev/nvme0n1 to create partitions with this layout:\n/dev/nvme0n1p1 - at least 512M - type EF00 - EFI System Partition /dev/nvme0n1p2 - rest of disk - type 8309 - LUKS Format the physical partitions EFI Partition mkfs.vfat -F 32 /dev/nvme0n1p1 LUKS Encrypted partition cryptsetup luksFormat /dev/nvme0n1p2 Create volume group and logical volumes First, open the encrypted container:\ncryptsetup luksOpen /dev/nvme0n1p2 luks As the result the encrypted partition is mounted at /dev/mapper/luks.\nNext, treat /dev/mapper/luks as an LVM PV and create your volumes. In my case are like:\nVolume Group vg0 Logical Volume lv_root - Probably at least 20G, I use 75G Logical Volume lv_swap - Optional, maybe not desirable if you have an SSD Logical Volume lv_home - Rest of the space The commands to achieve this:\n1 2 3 4 5 pvcreate /dev/mapper/luks vgcreate vg0 /dev/mapper/luks lvcreate -L 75G -n lv_root vg0 lvcreate -L 16G -n lv_swap vg0 lvcreate -l100%FREE -n lv_home vg0 Format the logical volumes I will use ext4 filesystems for my setup, here you can use something different (like btrfs).\nTo format root and home partitions in ext4:\n1 2 mkfs.ext4 /dev/vg0/lv_root mkfs.ext4 /dev/vg0/lv_home To format the swap partition and enable it:\n1 2 mkswap /dev/vg0/lv_swap swapon /dev/vg0/lv_swap Mount the partitions This step is required to mount the created partitions and install the Arch Linux system there. All the filesystems should be mounted considering /mnt as a root filesystem for the future installed system.\n1 2 3 mount --mkdir /dev/vg0/lv_root /mnt mount --mkdir /dev/vg0/lv_home /mnt/home mount --mkdir /dev/nvme0n1p1 /mnt/boot Install the base system pacstrap -K /mnt base base-devel linux linux-firmware linux-headers Generate the fstab file genfstab -U /mnt \u0026gt;\u0026gt; /mnt/etc/fstab Chroot into your system arch-chroot /mnt Generate locale Uncomment en_US.UTF-8 UTF-8 and other needed locales in file /etc/locale.gen.\nThen generate locales:\nlocale-gen To set the system locale:\necho \u0026#34;LANG=en_US.UTF-8\u0026#34; \u0026gt; /etc/locale.conf Setup the hostname This is actually the analog of computer name in Windows, in my case I will name it thinkpad.\necho \u0026#34;thinkpad\u0026#34; \u0026gt; /etc/hostname Also add the default values to the /etc/hosts file:\n1 2 3 4 # Static table lookup for hostnames. # See hosts(5) for details. 127.0.0.1 localhost ::1 localhost Setup TimeZone My timezone is Europe/Kiev, so in my case this sumlink should be created:\nln -s /usr/share/zoneinfo/Europe/Kiev /etc/localtime And also I recommend switch the BIOS hardware clock to UTC:\nhwclock --systohc --utc Setup initramfs Install the lvm2 package:\npacman -S lvm2 Edit the /etc/mkinitcpio.conf file and insert hooks encrypt and lvm2 strictly in this order between the block and filesystems hooks like this:\nHOOKS=(base udev ... block encrypt lvm2 filesystems) Then re-generate the initramfs:\nmkinitcpio -P Create a user and credntials First it is recommended to change the root user password:\npasswd root Then install sudo package to allow your user grant privileges:\npacman -S sudo Then edit the sudoers file:\nsudo EDITOR=nano visudo And uncomment the line %wheel ALL=(ALL:ALL) ALL and save the file.\nCreate a user, change the password and add it to the necessary groups:\n1 2 3 useradd -m shifthackz passwd shifthackz usermod -aG wheel,audio,video,storage shifthackz Install the needed packages and desktop environment This is optional step and you may do the same after install, but I\u0026rsquo;d like to be able to use the DE straigt after install.\nIn this example I will install Gnome DE (on Wayland and PipeWire) with NetworkManager.\npacman -S gnome networkmanager gnome pipewire \\ pipewire-alsa pipewire-pulse pipewire-jack \\ wireplumber bluez bluez-utils Then start the needed services by default\n1 2 3 systemctl enable NetworkManager systemctl enable gdm systemctl enable bluetooth Install the bootloader I will use systemd-boot as my bootloader, to install it, run:\nbootctl install Then create the bootloader config at /boot/loader/loader.conf containing this:\n1 2 3 4 default @saved timeout 3 console-mode max editor no To load your CPU microcode early at bootloader install amd-ucode or intel-ucode package, in my case I have Intel CPU, so command is:\npacman -S intel-ucode Then detect the UUID of your LVM encrypted partition (in my case /dev/nvme0n1p2):\nblkid /dev/nvme0n1p2 Then create the boot entry for your Arch Linux system at /boot/loader/entries/arch.conf, make sure to replace the UUID and correct root partition in the options parameter:\n1 2 3 4 5 title Arch Linux linux /vmlinuz-linux initrd /intel-ucode.img initrd /initramfs-linux.img options cryptdevice=UUID=b574960c-1d6a-4363-bd8a-0e7345f23e06:luks root=/dev/vg0/lv_root rw Finally check the bootctl and validate that the config is correct in bootctl list.\nReboot to your new system To reboot you need:\ntype exit to exit the chroot shell. then do umount -R /mnt to unmount your partitions. finally type reboot ","date":"2024-02-24T00:00:00Z","image":"http://localhost:1313/post/arch-linux-install-guide-uefi--encrypted-lvm/header_hue96a0cab901c05107ef04317fe9cfdbe_381439_120x120_fill_box_smart1_3.png","permalink":"http://localhost:1313/post/arch-linux-install-guide-uefi--encrypted-lvm/","title":"Arch Linux install guide (UEFI + encrypted LVM)"},{"content":"Introduction Usually WPS Office offers one of the best compatibility on Linux with weird proprietary ms office document formats including document with some complex formatting and formulas. But in order for the formulas to display correctly some custom fonts required, and by default they are missing on most of the Linux system. In that case after launching the WPS Office you will see the error message saying:\nSome formula symbols might not be displayed correctly due to missing fonts Symbol, Wingdings, Wingdings 2, Wingdings 3, Webdings, MT Extra.\nFix the fonts issue To fix the issue you just need to download the missing and install them in your Linux system.\nDownload fonts files Download needed fonts and save them in some folder. You can just click on every file below to download it:\nWEBDINGS.TTF WINGDNG2.ttf WINGDNG3.ttf mtextra.ttf symbol.ttf wingding.ttf Install downloaded fonts Create a folder that is required for formula fonts, by typing this command in the terminal:\nsudo mkdir -p /usr/share/fonts/kingsoft Then copy downloaded files to destination folder:\n1 2 3 4 5 6 sudo cp WEBDINGS.TTF /usr/share/fonts/kingsoft sudo cp WINGDNG2.ttf /usr/share/fonts/kingsoft sudo cp WINGDNG3.ttf /usr/share/fonts/kingsoft sudo cp mtextra.ttf /usr/share/fonts/kingsoft sudo cp symbol.ttf /usr/share/fonts/kingsoft sudo cp wingding.ttf /usr/share/fonts/kingsoft And make your user the owner of the folder:\nsudo chown -R $USER:$USER /usr/share/fonts/kingsoft Invalidate the font cache To invalidate your system font cache, run this command:\nsudo fc-cache -vfs Conclution After necessary fonts are installed close all the WPS office processes and try to launch it again, the error should not be shown after launch, and you should be able to use formula formatting.\n","date":"2023-12-15T00:00:00Z","image":"http://localhost:1313/post/fix-missing-formula-fonts-for-wps-office-on-linux/header_huca727e626e0e83a19f41e4803138b60e_99475_120x120_fill_box_smart1_3.png","permalink":"http://localhost:1313/post/fix-missing-formula-fonts-for-wps-office-on-linux/","title":"Fix missing formula fonts for WPS Office on Linux"},{"content":"Introduction It\u0026rsquo;s neat to have a laptop with both WiFi and LTE, as this actually provides a reserved connectivity channel. ThinkPads are generally well-supported on Linux, and many users successfully use LTE modems on these laptops without any additional configuration. However, some newer models are bundled with Quectel LTE modems that have FCC lock.\nFCC Lock What is FCC Lock The FCC lock is a software lock integrated in WWAN modules shipped by several different laptop manufacturers like Lenovo, Dell, or HP. This locks prevents the WWAN module from being put online until some specific unlock procedure (usually a magic command sent to the module) is done. You can read more details about the FCC lock/unlock procedure here.\nHow OS handles FCC Unlock On Windows based system FCC unlock is usually done by the driver provided by modem or laptop manufacturer. On Linux systems Modem Manager is used to operate the WWAN modem, and actually has some pre-defined scripts for certain well-known WWAN modems. In some cases manufacturer provides the FCC unlock scripts, especially when it comes to ThinkPads that are designed to be Linux-compatible in the first place. But as each modem has different way of FCC unlock procedure, the FCC unlock procedure is not enabled by default and user should manually enable FCC unlock script for the exact modem model and revision. This is the reason why LTE does not work out-of-box in Linux system, which may confuse some users.\nFCC Unlock example I have Lenovo ThinkPad X13 Gen2 with Intel CPU running Arch Linux, so this example shows how to perfom FCC unlock procedure with this exact laptop and OS environment. This procedure is quite similar for other Lenovo ThinkPad series laptops, but with your equipment procedure may be slightly different, so consider investigating manufacturer website or special forums regarding your exact hardware.\nDetect WWAN modem model First of all let\u0026rsquo;s detect the exact model of LTE modem that is installed in laptop. This can be done with terminal command:\nlspci It will actually print all the PCI devices in your system and you can look for the LTE modem in the output. In my case LTE modem was the last in the output.\n1 2 3 ... 08:00.0 Unassigned class [ff00]: Quectel Wireless Solutions Co., Ltd. EM120R-GL LTE Modem ... Install Modem Manager package Make sure you have installed modemmanager package, because it\u0026rsquo;s essential requirement to have it in order for LTE connectivity to work on your Linux system.\nDepending on your distribution, install command may be different.\nFor Arch-based systems: sudo pacman -S modemmanager For Debain-based systems: sudo apt install modemmanager For Fedora-based systems: sudo dnf install modemmanager After that enable the ModemManager.service in systemd, it can be done with command:\nsudo systemctl enable --now ModemManager.service Find the FCC Unlock script This step requires investiagting online to get the FCC Unlock script from the manufacturer. If there is no script from the manufacturer try to look on some forum pages or subreddits regarding your laptop/modem.\nIn my case I have Quectel EM120R-GL WWAN modem, and luckily the FCC unlock script is bundled with Modem Manager itself. I found the location of pre-defined script on this page.\nThe location of the pre-defined script for my modem is /usr/share/ModemManager/fcc-unlock.available.d/1eac:1001, and to make FCC unlock performing automatically with start of the ModemManager.service it\u0026rsquo;s enough to link this script to the /etc/ModemManager/fcc-unlock.d, so the command to do so look like:\nsudo ln -snf /usr/share/ModemManager/fcc-unlock.available.d/1eac:1001 /etc/ModemManager/fcc-unlock.d After this you can restart ModemManager.service:\nsudo systemctl restart ModemManager.service and try to connect to your LTE network, in case it not works try rebooting your computer.\nConnecting to the LTE network To perform connection to the LTE networks you can either use modemmanager in your terminal, or use some graphical settings tools provided by NetworkManager in KDE/Gnome. You can read detailed info how to perform mobile broadband connections on this arch wiki page.\nI will show both terminal and graphical way on my system.\nTerminal only method First you need to know the index of the WWAN modem. To do this, list all the available WWAN modems using mmcli like this:\nmmcli -L In the output look for the string /org/freedesktop/ModemManager1/Modem/1, the modem index is at the end, in my case it is 1. So this index should be used in all the below commands.\nNext, you can try connecting to LTE network, but you need to know what settings are required for your Internet Service Provider to perform connection. In my case only apn=internet is enough, so command to connect is:\nmmcli -m 1 --simple-connect=\u0026#34;apn=internet\u0026#34; If connection successful, try browsing the internet, or just do ping 8.8.8.8 in terminal to ensure you are online.\nAfter you done browsing, you can disconnect from LTE network using command:\nmmcli -m 1 --simple-connect=\u0026#34;apn=internet\u0026#34; Graphical method This method requires to have use desktop environment like KDE or Gnome, and have NetworkManager as the primary way of network setup in your Linux system. In this example I will show how to setup it on KDE Plasma 5.27.10.\nFirst of all go to the System Settings app, and open the Connections section.\nPress the + add button, and select Mobile Broadband connection type in the opened dialog.\nIn the next dialog you can select specific modem for this connection, but as I have only one WWAN modem in my system, I will leave Any GSM device selection as is.\nNext, select your ISP provider country.\nSelect your ISP provider, or you can enter the name manually if it\u0026rsquo;s not listed.\nOn this step select your tariff plan, and ensure you enter correct APN, in my case it is \u0026ldquo;internet\u0026rdquo;.\nAfter all the information was entered, you should see success window.\nKDE makes it easy to connect the LTE network and monitor its\u0026rsquo; state from the tray icon. Find the profile you just created and click connect, when you are online it should display connected status like on screenshot.\nConclusion So if you don\u0026rsquo;t have your LTE modem working out-of-box after installing Linux consider checking if it has FCC lock and look online for the unlock script. In case you have exactly the same Quectel EM120R modem like I have, you can just use the same FCC script from example.\n","date":"2023-12-13T00:00:00Z","image":"http://localhost:1313/post/how-to-make-thinkpad-lte-modem-work-on-arch-linux-using-fcc-unlock/header_hu2d109c230a8b683de6314808740ea101_247945_120x120_fill_q75_box_smart1.jpg","permalink":"http://localhost:1313/post/how-to-make-thinkpad-lte-modem-work-on-arch-linux-using-fcc-unlock/","title":"How to make ThinkPad LTE Modem work on Arch Linux using FCC unlock"},{"content":"Introduction Zsh, or Z Shell, is a powerful and feature-rich command-line shell for Unix-like operating systems, including Linux and BSD (Berkeley Software Distribution). It is an extended version of the Bourne Shell (sh) with numerous improvements and additional features. Zsh aims to provide a more interactive and user-friendly experience for shell users.\nHere are some reasons why Zsh is often considered one of the best shells for Unix/Linux/BSD systems:\nCommand Line Editing: Zsh offers advanced command-line editing capabilities, allowing users to navigate and edit commands with ease. It supports features like history expansion, spelling correction, and advanced pattern matching.\nCustomization: Zsh is highly customizable. Users can configure various aspects of the shell\u0026rsquo;s behavior, such as prompt appearance, key bindings, and completion options. The extensive configuration options make it adaptable to individual preferences and workflows.\nPowerful Tab Completion: Zsh provides powerful and context-aware tab completion, making it easier to navigate the file system and complete command names and arguments. It can complete not only commands but also file paths, variables, and more.\nPlugins and Extensions: Zsh supports plugins and extensions that enhance its functionality. Tools like Oh-My-Zsh and Prezto are popular frameworks that make it easy to manage Zsh configurations and add additional features through plugins.\nAdvanced Globbing: Zsh supports advanced globbing patterns, which provide more flexibility and power when specifying file paths or matching patterns. This makes it easier to perform complex file operations directly from the command line.\nSpelling Correction: Zsh has a built-in spelling correction feature that helps users avoid typos. If you mistype a command or file path, Zsh can suggest corrections.\nInteractive Features: Zsh includes interactive features that improve the overall user experience, such as the ability to navigate command history easily, search through previous commands, and reuse or modify commands efficiently.\nCompatibility with Bourne Shell: Zsh is compatible with the Bourne Shell (sh) syntax, making it a suitable replacement for sh or Bash. Existing shell scripts are likely to work in Zsh without modification.\nWhile Zsh offers a rich set of features, the choice of the \u0026ldquo;best\u0026rdquo; shell often depends on individual preferences and specific use cases. Other popular shells include Bash (Bourne Again SHell) and Fish (Friendly Interactive SHell), each with its own strengths and characteristics. Ultimately, the best shell is the one that aligns with your workflow and meets your specific requirements.\nInstallation Install ZSH package To install ZSH package in your Linux system consider using the package manager provided with your distribution. It is done differently depending on your distribution, for example:\nFor Arch Linux:\nsudo pacman -S zsh For Fedora, Red Hat:\nsudo dnf install zsh For Debian, Ubuntu, Linux Mint, ElementaryOS:\nsudo apt install zsh In my case, I am installing it for Arch Linux, for example:\nMake ZSH your default shell To make the new installed ZSH the default for your user, type the command below, and enter your user password for confirmation:\nchsh -s $(which zsh) After that it is recommended to reboot your system, on next reboot the ZSH shell will be used for your user. To reboot your Linux computer you can use command sudo reboot or just use GUI of your desktop environment to perform reboot.\nPerform the initial configuration Once you open the terminal application after reboot zsh will prompt you to create the default configuration files. To apply the initial configuration press \u0026ldquo;0\u0026rdquo; on the keyboard.\nInstall Oh My ZSH Oh My Zsh is an open-source framework and configuration manager for Zsh, the Z Shell. It was created to make it easier for users to manage their Zsh configurations and enhance their command-line experience. Oh-My-Zsh provides a collection of plugins, themes, and helper functions that can be easily integrated into Zsh, allowing users to customize and extend the functionality of their shell environment.\nTo install ZSH the most easy way is to use installation script from the Oh My Zsh GitHub Repository. It is possible to execute the installation script by downloading it, like this:\nsh -c \u0026#34;$(wget https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh -O -)\u0026#34; After script finishes, you should see Oh My Zsh installed like on screenshot below.\nModify Oh My ZSH theme Oh My Zsh has a plenty of pre-installed themes that are located in ~/.oh-my-zsh/themes/ folder.\nYou can modify your .zshrc file and use pre-installed theme.\nnano .zshrc And to change your theme to agnoster for example, find ZSH_THEME paramater and change it like this:\nZSH_THEME=\u0026#34;agnoster\u0026#34; Install custom Oh My ZSH theme Personally for me Powerlevel10k is the most liked among the others, so in this example I will demonstrate how to install it.\nInstall compatible terminal font In order for this custom theme to be able to correctly display some characters, your terminal should use a custom compatible fon Meslo LGS NF. To to this you can follow the instructions from the GitHub repository.\nThe most simple way is download and 4 font files:\nMesloLGS NF Regular.ttf MesloLGS NF Bold.ttf MesloLGS NF Italic.ttf MesloLGS NF Bold Italic.ttf To make them available system wide move those 4 files to /usr/share/fonts folder:\n1 2 3 4 sudo mv \u0026#34;MesloLGS NF Regular.ttf\u0026#34; /usr/share/fonts sudo mv \u0026#34;MesloLGS NF Bold.ttf\u0026#34; /usr/share/fonts sudo mv \u0026#34;MesloLGS NF Italic.ttf\u0026#34; /usr/share/fonts sudo mv \u0026#34;MesloLGS NF Bold Italic.ttf\u0026#34; /usr/share/fonts Finally, to flush the font cache, execute the command:\nsudo fc-cache The last step is to change the font of your terminal. It can depend on the terminal you are using, but usually you can just modify it in the terminal graphical settings. In my case I set it up to use Meslo LGS NF Regular 12.\nInstall Powerlevel10k theme First download the Powerlevel10k theme to your custom themes directory, it can be done easily just with one command:\ngit clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k Then modify your .zshrc file:\nnano .zshrc and set ZSH_THEME parameter to be equal to powerlevel10k/powerlevel10k:\nZSH_THEME=\u0026#34;powerlevel10k/powerlevel10k\u0026#34; After you saved the file, use the command below to apply the changes:\nsource ~/.zshrc Complete Powerlevel10k configuration wizard The first time new custom Powerlevel10k is running, the Powerlevel10k theme configuration wizard will start. You can always start this wizard again by using p10k configure terminal command. The wizard has several steps, just press corresponding keyboard keys to answer. I will show below how the configuration looks like in my case.\n1. Confirm that several font symbols are displayed correctly. I just answered yes to those questions (by pressing \u0026ldquo;y\u0026rdquo; on keyboard), because everything in terms of symbol rendering was displayed correctly.\n2. Setup theme style All the options in this section allow to modify the design of the theme according to your personal preferences. I will show what answers I typed into the wizard in case you want to replicate the exact theme behavior from this guide.\nThe prompt style setting will allow to setup the main look and feel, I answered 3 here. The character set is important setting, and actually Unicode allows to display more symbols, so I highly recommend answer 1 here. This setting allows to show or hide current time in terminal. I see no practical use in this, so I answered n here. The next settings are all about the look and feel of the theme, you can choose whatever you personally like.\nThe instant prompt feature allows to reduce the loading times of the ZSH shell, so I highly recomment to enable it by answering 1 here. The last step is to apply changes to .zshrc, just answer y here. If everything was set up right, you will see your new shell theme.\nEnable built-in Oh My ZSH plugins Zsh supports plugins and extensions that enhance its functionality. Oh-My-Zsh makes it easy to manage Zsh configurations and add additional features through plugins.\nTo view a list of plugins that is bundeled with Oh My ZSH, you can type a command:\nls -a ~/.oh-my-zsh/plugins/ In my case I have those plugins:\nThen you can modify your .zshrc file:\nnano .zshrc and set plugins parameter to load the plugins you need, in my case as an Android Developer I need git and adb plugins:\nplugins=(git adb) Then apply the changes to your shell:\nsource ~/.zshrc Installing custom plugins Also Oh My ZSH allows you to install and user third party plugins. For example let\u0026rsquo;s install zsh-autosuggestions plugins to have some nice recommendations for input commands based on history.\nInstalling is very similar to installing a custom theme. First, clone the plugin into the custom plugins directory.\ngit clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions Then you can modify your .zshrc file the same way as I shown in the article section above, so in the result you will have:\nplugins=(git adb zsh-autosuggestions) Then apply the changes to your shell:\nsource ~/.zshrc After that try to type something, and you should see some suggestion. For example I typed \u0026ldquo;neo\u0026rdquo;, and the plugin suggests to use \u0026ldquo;neofetch\u0026rdquo;:\nEnabling auto updates Oh-My-Zsh has a built-in update mechanism to help users keep their installation up-to-date with the latest changes, enhancements, and bug fixes contributed by the community. Also it provides a way for automatic updates, but it is disabled by default.\nTo configure automatic updates you should set this directive in .zshrc file:\nzstyle \u0026#39;:omz:update\u0026#39; mode auto Conclusions So ZSH is more feature rich shell that can make you work more efficient with extending the shell functionality with custom plugins, so it can be considered as a better alternative to bash that is the default shell in any modern Linux distribution.\n","date":"2023-11-26T00:00:00Z","image":"http://localhost:1313/post/how-to-install-and-configure-zsh-shell-in-linux/header_hu6914af8cdb8921216861b6ef227624ec_122422_120x120_fill_q75_box_smart1.jpg","permalink":"http://localhost:1313/post/how-to-install-and-configure-zsh-shell-in-linux/","title":"How to install and configure ZSH shell in Linux"},{"content":"Introduction When using systemd-boot as your bootloader, you may find it convenient to have the system remember the last selected entry on each subsequent boot. This is especially useful for users who frequently switch between different operating systems or kernels. By configuring systemd-boot to remember the last chosen boot entry, you can streamline the boot process and avoid having to manually select the desired option every time the system restarts.\nModifying the Configuration File To achieve this functionality, you need to modify the loader.conf configuration file. The exact location of this file can vary depending on the Linux distribution you are using.\nPersonally, I have used several Linux distributions, and the path for loader.conf was different in each of them, for example:\nFor Ubuntu it was /boot/efi/loader/loader.conf For Arch Linux it was /boot/loader/loader.conf For EndeavourOS it was /efi/loader/loader.conf To modify the file open the Terminal and follow this steps:\nOpen the loader.conf file for editing, for example: sudo nano /boot/loader/loader.conf Modify the default parameter like below: default @saved Save the file (In nano, this is done with the keyboard shortcut `Ctrl + O\u0026rsquo;). At the next boot, after you will select some entry it will be remembered as the default.\nConclusions By configuring systemd-boot to remember the last selected entry, you can streamline your boot process and enhance the overall user experience. Whether you\u0026rsquo;re using Arch Linux, Ubuntu, or another distribution that employs systemd-boot, this simple modification can save you time and make your system startup more efficient.\nRemember to adapt the file paths and commands based on the specifics of your distribution. With this configuration in place, your system will automatically boot into the last chosen entry, reducing the need for manual intervention during the boot process.\n","date":"2023-11-22T00:00:00Z","image":"http://localhost:1313/post/how-to-make-systemd-boot-remember-the-last-selected-entry/header_hu0e765b8aca8d9ba54a80770b06516939_1619_120x120_fill_box_smart1_3.png","permalink":"http://localhost:1313/post/how-to-make-systemd-boot-remember-the-last-selected-entry/","title":"How to make systemd-boot remember the last selected entry"},{"content":"Introduction Currently, there are many platforms for online video content streaming, such as YouTube, Twitch, and others. They use the RTMP protocol (Real-Time Messaging Protocol) to broadcast streaming video over the Internet. While these platforms have powerful video streaming capabilities, in some cases independence from the streaming platform and its rules makes perfect sense.\nIn this article, I will provide instructions on the deployment of the RTMP service based on Nginx-RTMP, which will allow receiving an RTMP stream from the streamer\u0026rsquo;s computer, and converting it to modern HLS and DASH formats for viewing in the receiver program.\nPrerequisites To deploy the RTMP service, you need to have:\nA clean virtual machine or physical server based on the Linux OS. A computer for streaming. To work outside the local network, in the global Internet:\nDedicated IP address. Domain. The instructions will use a VPS based on Debian 11.\nWorking with Nginx-RTMP Installation First of all, you need to install nginx and libnginx-mod-rtmp packages. To do this, you need to execute the following commands:\n1 2 sudo apt update sudo apt install nginx libnginx-mod-rtmp Set up RTMP service Once installed, you need to configure the Nginx web server to listen on port 1935 to receive the RTMP stream. To do this, you need to edit the file /etc/nginx/nginx.conf:\nsudo nano /etc/nginx/nginx.conf At the end of the file, you need to add the configuration of the RTMP server:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ... rtmp { server { listen 1935; chunk_size 4096; allow publish 127.0.0.1; allow publish 192.168.0.0/24; deny publish all; application live { live on; record off; hls on; hls_path /var/www/html/stream/hls; hls_fragment 3; hls_playlist_length 60; dash on; dash_path /var/www/html/stream/dash; } } } ... An explanation of the important aspects of this configuration:\nlisten 1935 - specifies the port on which the RTMP server is running. chunk_size 4096 - sets the block size, 4 KB each. allow publish [IP / Subnet] - each line specifies an IP or subnet that is allowed to send the RTMP stream to the server. deny publish all - prohibits receiving RTMP stream from all other addresses/networks. application live - configuration for converting RTMP to HLS and DASH formats, where hls_path and dash_path indicate paths to directories for placing playlists. live on - allows you to receive data via a video stream. record off - disables recording of the video stream to a file on the disk. Set up streaming HLS, DASH Next, you need to deploy a virtual host that will allow access to HLS or DASH streams via the HTTP/HTTPS protocol.\nFirst, you need to create two directories for storing video stream fragments for HLS and DASH:\n1 2 sudo mkdir -p /var/www/html/stream/hls sudo mkdir -p /var/www/html/stream/dash And also establish the owner and rights:\n1 2 sudo chown -R www-data:www-data /var/www/html/stream sudo chmod -R 755 /var/www/html/stream For the virtual host to work, you need to create a new configuration file (for example rtmp) in the /etc/nginx/sites-available directory:\nsudo nano /etc/nginx/sites-available/rtmp The rtmp host file consists of the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 server { listen 443 ssl; listen 80; server_name rtmp.yourdomain.com; ssl_certificate /etc/ssl/yourdomain.crt; ssl_certificate_key /etc/ssl/yourdomain.key; ssl_protocols SSLv3 TLSv1 TLSv1.1 TLSv1.2; location / { add_header Access-Control-Allow-Origin *; root /var/www/html/stream; } } types { application/dash+xml mpd; } Explanation for this configuration:\nReplace rtmp.yourdomain.com with your domain. If you want to use SSL, also write the certificate and key files under the paths /etc/ssl/yourdomain.crt and /etc/ssl/yourdomain.key. If you don\u0026rsquo;t want to use SSL, remove the lines starting with ssl and listen 443 ssl from the file. In order for all the configurations to take effect, you need to enable the virtual host and restart Nginx:\n1 2 sudo ln -s /etc/nginx/sites-available/rtmp /etc/nginx/sites-enabled/ sudo service nginx restart Broadcasting in OBS Studio The best suited program for broadcasting is OBS Studio.\nFirst of all, it is necessary to create a scene, adjust the sound, and the appearance of the broadcast.\nTo configure streaming parameters, you need to go to Settings and select the Stream tab. The following parameters must be set there:\nService: Custom Server: rtmp://rtmp.yourdomain.com/live (instead of domain, you can specify IP, for example http://11.22.33.44/live) Stream Key: obs_stream Example settings:\nTo start a video broadcast, you need to click Start Streaming in the main window of the program:\nViewing the broadcast The broadcast can now be viewed using any application that supports the HLS and DASH protocols. The easiest way would be to watch in VLC by opening the link to the stream.\nTo begin with, let\u0026rsquo;s understand how a link to streams is formed in the configured service:\nHLS: {protocol}://{domain}/hls/{stream key}.m3u8 DASH: {protocol}://{domain}/dash/{stream key}.mpd For example, if you deployed a service at the address rtmp.yourdomain.com that uses SSL, and in the OBS settings you specified the obs_stream key, then in this case the links will be as follows:\n1 2 https://rtmp.yourdomain.com/hls/obs_stream.m3u8 https://rtmp.yourdomain.com/dash/obs_stream.mpd To view in VLC, you need to press Ctrl + N, or go to the menu Media \u0026gt; Open Network Stream, specify a link to one of the formats, and press Play.\nConclusions In this way, you can create your own broadcasting service, which will be independent of popular services.\nAdvantages of such a solution:\nPrivacy and full control over the infrastructure, guarantee that the flow data is not stored. You do not need to follow the rules of the service (for example, a ban on broadcasting certain content). But there are certain disadvantages:\nSuch a solution requires certain server resources. The owner needs to spend time and money on maintaining and maintaining the security of his infrastructure. ","date":"2022-07-20T00:00:00Z","image":"http://localhost:1313/post/deploying-an-rtmp-server-for-streaming-using-nginx-rtmp/header_hu41968337912a42023606e3de60723367_1230034_120x120_fill_q75_box_smart1.jpeg","permalink":"http://localhost:1313/post/deploying-an-rtmp-server-for-streaming-using-nginx-rtmp/","title":"Deploying an RTMP server for streaming using Nginx RTMP"},{"content":"Introduction Tor is an open source software complex that connects certain computers around the world into a system of proxy servers, the connections between which are made according to the bulb routing scheme. This allows users of this network to establish an anonymous connection that is protected from eavesdropping.\nThe main purpose of the Tor network today is to provide anonymity for Internet users, allowing you to hide your identity while browsing sites from ISPs, site owners, advertising robots, automated traffic analysis systems, etc. This is achieved due to a large distributed system of servers - nodes, traffic between which is routed at the network level according to the OSI model.\nRoles of Tor network subjects Knowing that the network consists of server-nodes connected to the bulb network, let\u0026rsquo;s consider which nodes are in the Tor network:\nEntry Node (Entry Node) The input node is the first link in the connection chain. It initiates the establishment of a secure connection by accepting packets from a Tor user, encrypts them, and forwards them to the next node. Note that interception of data between the user and the input node is impossible, since each block is encrypted with a session key using hybrid encryption.\nMiddle Node The task of these nodes is reduced only to receiving data from the previous node, encrypting it and transmitting it to the next one. From such a site, it is impossible to go outside the Tor network to the Internet. From such a node, you can only get to the site of the intranet domain .onion, no more. But these nodes are very important to keep the network functioning: the more intermediaries in the chain, the higher the anonymity, and the probability of your connection being compromised decreases. In addition, it is impossible to establish through which intermediary nodes your chain passes, since the IP addresses of such nodes are not recorded in the log file.\nExit Node This is the last node in the Tor chain. It decodes the packets transmitted by the user along the entire chain and transmits the data to the requested remote server on the Internet. A connection from the IP address of the source node will be registered on the requested server. These types of nodes are the weakest point of the Tor network, as there are ways to intercept user data between the origin node and the remote server. Also, a volunteer who has launched an Exit Node exposes himself to the risk of problems, since it is his IP that is fixed by Internet resources.\nBridge Relay These are repeaters whose addresses are publicly available. They are used to initiate connections in places where Tor root server addresses are blocked. You can get the address of the bridge node by accepting an e-mail from TheTorProject with a special request. Thus, even a total blocking of all publicly known addresses of Tor network nodes will not give any result, since it will not affect the availability of classified relays.\nExit Enclave A relay used by site owners to create a mirror of their resource on the Tor network. This will allow users of some countries to bypass blocking, and the rest to maintain anonymity, protecting themselves again from interception of traffic from the source node.\nWhat the chain between the user and the final Internet resource looks like is shown in the diagram below:\nFeatures of Tor Also, Tor, since 2004, can provide anonymity for servers as well. Each network user can host any service, the so-called .onion internal domain, which consists of a random set of characters. Such a service will be available only from the Tor network, and neither users will be able to find out about the public IP of the hidden site, nor the site administrator will be able to know who its visitors are. Only the fact of visiting the site and the specific directory visited by the user will be available in the server logs.\nBut you need to be careful when using Tor, because it has the following features that should be kept in mind when using Tor:\nThe input node knows the user\u0026rsquo;s IP address; The message is fully decoded at the source node, but the sender is unknown; On the requested remote server, some technical data about the sender is transmitted along with the package, although the sender is essentially unknown; Packets with your message may pass through nodes that were launched by attackers in order to steal your data; The Internet resource can obtain data about the user\u0026rsquo;s PC configuration and OS version, as well as his IP address by executing JavaScript, Flash, ActiveX scripts on the page; On the way from the source node to the final server, data can be changed due to a Man-In-The-Middle attack, so if, for example, you download a file through Tor, always check its hash sums; The owner of the source node can steal your session, cookies, and even logins and passwords by intercepting data using SSL Strip; Since the IP addresses of the outgoing nodes are publicly available, some Internet providers and sites block them (for example, Google, VK); Conclusions We can conclude that the Tor network has great value for those who are subject to mass surveillance, for those who value freedom on the Internet, for those who do not want to put up with large-scale senseless blocking, for those who want at least a small island of anonymity .\nBe careful on the internet, because it\u0026rsquo;s possible that right now, as you read this, someone is watching your traffic ðŸ˜±.\n","date":"2016-10-07T00:00:00Z","image":"http://localhost:1313/post/how-the-tor-network-actually-works/header_hu447e204f4d18572619e9ad6f34ad1dde_86330_120x120_fill_q75_box_smart1.jpg","permalink":"http://localhost:1313/post/how-the-tor-network-actually-works/","title":"How the Tor network actually works"},{"content":"Introduction Mega.nz is one of the most affordable cloud storage in terms of volume, because it provides its new users with 50Gb of cloud disk space absolutely free. There are also paid plans that allow you to expand the cloud up to 4 terabytes. But even 50Gb is quite enough for backup copies of sites and MySQL databases. Also, there is a set of megatools console utilities for downloading and uploading files to a remote cloud.\nSolution setup Installing megatools First, register and activate your mega.nz account if you don\u0026rsquo;t already have one.\nNext, you need to connect to the server via SSH, and install the necessary packages for assembling megatools:\nsudo apt-get -y install build-essential libglib2.0-dev libssl-dev libcurl4-openssl-dev libgirepository1.0-dev After that, you should find a link to download megatools on the official website, which we then use to download with the wget command.\n1 2 3 cd /opt wget https://megatools.megous.com/builds/megatools-1.9.97.tar.gz tar -xvzf megatools-1.9.97.tar.gz After we have downloaded and unzipped the source code, we need to compile it. This can be done using the following sequence of commands:\n1 2 3 4 cd megatools-1.9.97 ./configure make make install If everything was compiled and installed without errors, you can proceed to the next stage, namely writing a script for creating and uploading backups to the cloud.\nCreating a backup script First, we create a file with data for logging into the account:\n1 2 cd ~ nano .megarc The file should be filled as follows:\n1 2 3 [Login] Username = {Your login} Password = {Your password} As our login data is stored unencrypted, let\u0026rsquo;s make it available only to root.\nchmod 640 .megarc Now let\u0026rsquo;s check the correctness of entering the login and password, for this we enter the command:\nmegals If all settings are correct, it should display a list of files. If the command did not display a list of files, then we check the correctness of entering the password, if it did, then we proceed to the next step of creating a backup script. In this case, the scripts are stored in the /opt/scripts directory with modified permissions.\nnano /opt/scripts/do_backup.sh The script looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 #!/bin/bash SERVER=\u0026#34;server\u0026#34; DAYS_TO_BACKUP=7 WORKING_DIR=\u0026#34;/root/tmp_dir\u0026#34; BACKUP_MYSQL=\u0026#34;true\u0026#34; MYSQL_USER=\u0026#34;{Your MySQL user}\u0026#34; MYSQL_PASSWORD=\u0026#34;{Your MySQL password}\u0026#34; DOMAINS_FOLDER=\u0026#34;/var/www\u0026#34; ################################## # We create a temporary folder for creating archives rm -rf ${WORKING_DIR} mkdir ${WORKING_DIR} cd ${WORKING_DIR} # Archive /etc cd / tar cJf ${WORKING_DIR}/etc.tar.gx etc cd - \u0026gt; /dev/null # Backup MySQL if [ \u0026#34;${BACKUP_MYSQL}\u0026#34; = \u0026#34;true\u0026#34; ] then mkdir ${WORKING_DIR}/mysql for db in $(mysql -u${MYSQL_USER} -p${MYSQL_PASSWORD} -e \u0026#39;show databases;\u0026#39; | grep -Ev \u0026#34;^(Database|mysql|information_schema|performance_schema|phpmyadmin)$\u0026#34;) do #echo \u0026#34;processing ${db}\u0026#34; mysqldump --opt -u${MYSQL_USER} -p${MYSQL_PASSWORD} \u0026#34;${db}\u0026#34; | gzip \u0026gt; ${WORKING_DIR}/mysql/${db}_$(date +%F_%T).sql.gz done #echo \u0026#34;all db now\u0026#34; mysqldump --opt -u${MYSQL_USER} -p${MYSQL_PASSWORD} --events --ignore-table=mysql.event --all-databases | gzip \u0026gt; ${WORKING_DIR}/mysql/ALL_DATABASES_$(date +%F_%T).sql.gz fi # Backup websites mkdir ${WORKING_DIR}/domains for folder in $(find ${DOMAINS_FOLDER} -mindepth 1 -maxdepth 1 -type d) do cd $(dirname ${folder}) tar cJf ${WORKING_DIR}/domains/$(basename ${folder}).tar.xz $(basename ${folder}) cd - \u0026gt; /dev/null done ################################## # Handling possible dbus-errors export $(dbus-launch) # Create a folder on the cloud with the name of the server, and in it another folder with today\u0026#39;s date [ -z \u0026#34;$(megals --reload /Root/backup_${SERVER})\u0026#34; ] \u0026amp;\u0026amp; megamkdir /Root/backup_${SERVER} # Cleaning old unnecessary logs while [ $(megals --reload /Root/backup_${SERVER} | grep -E \u0026#34;/Root/backup_${SERVER}/[0-9]{4}-[0-9]{2}-[0-9]{2}$\u0026#34; | wc -l) -gt ${DAYS_TO_BACKUP} ] do TO_REMOVE=$(megals --reload /Root/backup_${SERVER} | grep -E \u0026#34;/Root/backup_${SERVER}/[0-9]{4}-[0-9]{2}-[0-9]{2}$\u0026#34; | sort | head -n 1) megarm ${TO_REMOVE} done # Create a folder for backup curday=$(date +%F) megamkdir /Root/backup_${SERVER}/${curday} 2\u0026gt; /dev/null # Upload the files to cloud megacopy --reload --no-progress --local ${WORKING_DIR} --remote /Root/backup_${SERVER}/${curday} \u0026gt; /dev/null # Kill DBUS-daemon kill ${DBUS_SESSION_BUS_PID} rm -f ${DBUS_SESSION_BUS_ADDRESS} # Remove temporary files rm -rf ${WORKING_DIR} exit 0 Now you need to allow the execution of the script:\nchmod a+x /opt/scripts/do_backup.sh Next, you need to test the script by directly executing it:\n/opt/scripts/do_backup.sh After that, you can go to the mega account through the web interface and check that the necessary files have appeared there.\nCreating a script autorun rule in crontab In order for the script to run according to a certain time schedule, let\u0026rsquo;s add it to the crontab.\n04 04 * * * root /opt/scripts/do_backup.sh Is it optimal to use ? In my case, the backup folder is 538.8 Mb in size.\nA total of 50,000 Mb of free space on the cloud. Let each backup weigh approximately 550 Mb. We divide 50,000 by 550, we have:\n50000 / 550 â‰ˆ 90.9 This means that the cloud is enough for 90 backups, which is quite a large number, especially if you take into account the free service of Mega.\nBut optimality as a whole depends on factors:\nBackup size Backup frequency Storage duration of each backup Therefore, it is advisable to evaluate the optimality separately for each individual case.\n","date":"2016-10-02T00:00:00Z","image":"http://localhost:1313/post/automatic-server-backup-to-the-mega.nz-cloud/header_hu07d9fa8bf00f81164ad47a9c5dedba00_591360_120x120_fill_q75_box_smart1.jpg","permalink":"http://localhost:1313/post/automatic-server-backup-to-the-mega.nz-cloud/","title":"Automatic server backup to the Mega.nz cloud"},{"content":"Introduction SSH Tunnels allow you to forward specific ports on a remote server or locally. This is very convenient when we need to get to a specific server in the local network.\nTechnically, it is possible to forward both local and remote ports. We will consider both cases.\nLocal port forwarding Let\u0026rsquo;s imagine the situation when we are inside a local network, where access to the Internet is blocked by a firewall for all but one server that has direct access to the Internet. We have SSH access to this server. Our task is to connect to a remote server that is on an external SSH network.\nFor example:\nssh -f -N -L 2222:212.212.212.212:22 user@111.111.111.111 This command will create a tunnel by opening port 22 of the remote server through the local server, and we can connect to the remote server through port 2222, which will listen on the local interface of our PC.\nWe should leave the terminal with the tunnel session running, and in the new terminal we can connect to the remote server with the command:\nssh -p2222 127.0.0.1 Finally, we have SSH access to the remote server.\nRemote port forwarding This case is the opposite of local port forwarding. Let\u0026rsquo;s imagine the same local network and remote server, only now the local PC has access to the Internet through NAT. Let\u0026rsquo;s say that a system administrator who has physical access to a remote server needs to RDP to computer 192.168.0.2, but NAT will not allow him to do so directly.\nConsider an example where there is an RDP service that by default is running on local port 3389. Let\u0026rsquo;s send it to remote port 3333.\nssh -f -N -R 3333:127.0.0.1:3389 username@212.212.212.212 After setting up such a tunnel, the sysadmin sitting behind the remote server will be able to connect to us by RDP using the address 127.0.0.1:3333 in the RDP client.\nConclusions These simple techniques of tunneling through the SSH protocol allow you to redirect the ports of a local or remote service as you like, which can be useful if you need to bypass certain network restrictions, such as NAT.\n","date":"2016-09-26T00:00:00Z","image":"http://localhost:1313/post/port-forwarding-using-an-ssh-tunnel/header_hu9370f1f3ff0baab462d9f98d73fd8fc5_63678_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"http://localhost:1313/post/port-forwarding-using-an-ssh-tunnel/","title":"Port forwarding using an SSH tunnel"}]